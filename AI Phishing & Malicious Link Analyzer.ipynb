{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc3ced-193a-43bb-8431-bb5063e994c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement onnxruntime==1.19.2 (from versions: 1.20.0, 1.20.1, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.23.0, 1.23.1, 1.23.2)\n",
      "ERROR: No matching distribution found for onnxruntime==1.19.2\n"
     ]
    }
   ],
   "source": [
    "# If running inside Jupyter, use the ! prefix to run shell commands\n",
    "!python -m pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "# Core ML + utilities\n",
    "!pip install -q scikit-learn==1.6.0 numpy pandas joblib tqdm matplotlib seaborn shap==0.46.0 \\\n",
    "    tldextract uritools python-Levenshtein\n",
    "\n",
    "# API + serving\n",
    "!pip install -q fastapi==0.115.5 uvicorn[standard]==0.32.0 pydantic==2.9.2 python-multipart\n",
    "\n",
    "# Optional: ONNX runtime for a “Local mode” inference toggle\n",
    "!pip install -q onnxruntime==1.19.2\n",
    "\n",
    "# Optional: Optuna for quick tuning if time permits\n",
    "!pip install -q optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af36f457-1790-4024-b221-d1526748871c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q onnxruntime==1.23.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e7e19c-cae2-4229-9523-3bc1faba2726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                  URL  URLLength                      Domain  \\\n",
       " 0    https://www.southbankmosaics.com         31    www.southbankmosaics.com   \n",
       " 1            https://www.uni-mainz.de         23            www.uni-mainz.de   \n",
       " 2      https://www.voicefmradio.co.uk         29      www.voicefmradio.co.uk   \n",
       " 3         https://www.sfnmjournal.com         26         www.sfnmjournal.com   \n",
       " 4  https://www.rewildingargentina.org         33  www.rewildingargentina.org   \n",
       " \n",
       "    DomainLength  IsDomainIP  TLD  TLDLength  NoOfSubDomain  HasObfuscation  \\\n",
       " 0            24           0  com          3              1               0   \n",
       " 1            16           0   de          2              1               0   \n",
       " 2            22           0   uk          2              2               0   \n",
       " 3            19           0  com          3              1               0   \n",
       " 4            26           0  org          3              1               0   \n",
       " \n",
       "    NoOfObfuscatedChar  ...  LetterRatioInURL  NoOfDegitsInURL  \\\n",
       " 0                   0  ...             0.581                0   \n",
       " 1                   0  ...             0.391                0   \n",
       " 2                   0  ...             0.517                0   \n",
       " 3                   0  ...             0.500                0   \n",
       " 4                   0  ...             0.606                0   \n",
       " \n",
       "    DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  \\\n",
       " 0              0.0                0               0                   0   \n",
       " 1              0.0                0               0                   0   \n",
       " 2              0.0                0               0                   0   \n",
       " 3              0.0                0               0                   0   \n",
       " 4              0.0                0               0                   0   \n",
       " \n",
       "    NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS  label  \n",
       " 0                           1                  0.032        1      1  \n",
       " 1                           2                  0.087        1      1  \n",
       " 2                           2                  0.069        1      1  \n",
       " 3                           1                  0.038        1      1  \n",
       " 4                           1                  0.030        1      1  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       " label\n",
       " 1    0.571895\n",
       " 0    0.428105\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook cell: load data and select features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"PhiUSIIL_Phishing_URL_Dataset.csv\")  # adjust if different\n",
    "use_cols = [\n",
    "    \"URL\", \"Domain\", \"TLD\", \"URLLength\", \"DomainLength\", \"TLDLength\",\n",
    "    \"IsHTTPS\", \"IsDomainIP\", \"NoOfSubDomain\",\n",
    "    \"NoOfLettersInURL\", \"NoOfDegitsInURL\", \"DegitRatioInURL\", \"LetterRatioInURL\",\n",
    "    \"NoOfQMarkInURL\", \"NoOfAmpersandInURL\", \"NoOfEqualsInURL\", \"NoOfOtherSpecialCharsInURL\",\n",
    "    \"SpacialCharRatioInURL\", \"HasObfuscation\", \"NoOfObfuscatedChar\", \"ObfuscationRatio\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, usecols=use_cols)\n",
    "df = df.dropna()  # dataset generally has no missing, but safe\n",
    "df.head(), df['label'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00acf31-b88f-4df1-ab0a-3bd7dbfc81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((188636, 19), (47159, 19))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Collapse rare TLDs\n",
    "tld_counts = df[\"TLD\"].fillna(\"unknown\").value_counts()\n",
    "rare = set(tld_counts[tld_counts < 50].index)\n",
    "df[\"TLD\"] = df[\"TLD\"].fillna(\"unknown\").apply(lambda x: \"other\" if x in rare else x)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\", \"URL\", \"Domain\"]]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "# Ensure categorical dtype for TLD (for tree support or for one-hot)\n",
    "X[\"TLD\"] = X[\"TLD\"].astype(\"category\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada8f6fa-38e5-48a1-83f5-dbd8a24f6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python313\\Lib\\site-packages\\sklearn\\calibration.py:337: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': np.float64(0.9989957719351902), 'AP': np.float64(0.9988523965504068), 'F1@0.5': 0.9978532035385128, 'Prec@90%Rec': 0.5718950783519583, 'Thr@90%Rec': 0.9999865414941713}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9995    0.9948    0.9971     20189\n",
      "           1     0.9961    0.9996    0.9979     26970\n",
      "\n",
      "    accuracy                         0.9975     47159\n",
      "   macro avg     0.9978    0.9972    0.9975     47159\n",
      "weighted avg     0.9975    0.9975    0.9975     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train + evaluate\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, average_precision_score, classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "\n",
    "# Identify categorical features (only TLD here)\n",
    "categorical_features = [X_train.columns.get_loc(\"TLD\")]\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=None,\n",
    "    max_leaf_nodes=31,\n",
    "    learning_rate=0.08,\n",
    "    max_iter=300,\n",
    "    validation_fraction=0.1,\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    categorical_features=categorical_features\n",
    ")\n",
    "\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate probabilities (sigmoid) on validation split for better dial behavior\n",
    "cal = CalibratedClassifierCV(hgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "# Metrics\n",
    "proba = cal.predict_proba(X_val)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val, proba)\n",
    "ap = average_precision_score(y_val, proba)\n",
    "f1 = f1_score(y_val, pred)\n",
    "\n",
    "# Precision@90% recall (safety posture)\n",
    "prec, rec, th = precision_recall_curve(y_val, proba)\n",
    "target_recall = 0.90\n",
    "idx = np.where(rec >= target_recall)[0]\n",
    "p_at_90 = float(prec[idx[0]]) if len(idx) else float(\"nan\")\n",
    "thr_at_90 = float(th[idx[0]-1]) if len(idx) else float(\"nan\")\n",
    "\n",
    "print({\"AUC\": auc, \"AP\": ap, \"F1@0.5\": f1, \"Prec@90%Rec\": p_at_90, \"Thr@90%Rec\": thr_at_90})\n",
    "print(classification_report(y_val, pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c90e497-ee84-4f49-ac0c-6c5c77f8e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models/phish_url_hgb_cal.joblib and models/schema.json\n"
     ]
    }
   ],
   "source": [
    "# STEP J: Persist the calibrated model to disk for reuse in API\n",
    "import joblib, json, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(cal, \"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "# STEP K: Save the exact schema: column order, categorical columns, and TLD categories\n",
    "schema = {\n",
    "    \"feature_cols\": list(X_train.columns),          # exact order used by the model\n",
    "    \"categorical_cols\": [\"TLD\"],\n",
    "    \"tld_categories\": list(X_train[\"TLD\"].cat.categories)\n",
    "}\n",
    "json.dump(schema, open(\"models/schema.json\", \"w\"))\n",
    "\n",
    "print(\"Saved: models/phish_url_hgb_cal.joblib and models/schema.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8380740-397d-432c-aa16-359266fb91cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created scripts/featureizer.py\n",
      "File exists: True\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts.featureizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFile exists:\u001b[39m\u001b[33m\"\u001b[39m, os.path.exists(\u001b[33m\"\u001b[39m\u001b[33mscripts/featureizer.py\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Now import\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatureizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m features_from_url\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImport successful!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scripts.featureizer'"
     ]
    }
   ],
   "source": [
    "# STEP N0b: Create scripts/featureizer.py from scratch\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Make sure scripts folder exists\n",
    "Path(\"scripts\").mkdir(exist_ok=True)\n",
    "\n",
    "# Write the featureizer module\n",
    "featureizer_code = r'''import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "RARE_TLD = set()\n",
    "TLD_CATS = []\n",
    "\n",
    "def _count(pattern, s):\n",
    "    return len(re.findall(pattern, s))\n",
    "\n",
    "def features_from_url(url: str):\n",
    "    u = (url or \"\").strip()\n",
    "    parsed = urlparse(u)\n",
    "    scheme = parsed.scheme or \"\"\n",
    "    ext = tldextract.extract(u)\n",
    "    domain = \".\".join([p for p in [ext.subdomain, ext.domain, ext.suffix] if p])\n",
    "    tld = ext.suffix or \"unknown\"\n",
    "\n",
    "    url_len = len(u)\n",
    "    domain_len = len(domain)\n",
    "    tld_len = len(tld)\n",
    "    is_https = 1 if scheme.lower() == \"https\" else 0\n",
    "    is_ip = 1 if re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", ext.domain or \"\") else 0\n",
    "\n",
    "    subdomain = ext.subdomain or \"\"\n",
    "    no_sub = 0 if not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "    letters = _count(r\"[A-Za-z]\", u)\n",
    "    digits = _count(r\"[0-9]\", u)\n",
    "    others = _count(r\"[^A-Za-z0-9]\", u)\n",
    "\n",
    "    letter_ratio = round(letters / url_len, 3) if url_len else 0.0\n",
    "    digit_ratio  = round(digits / url_len, 3) if url_len else 0.0\n",
    "    special_ratio = round(others / url_len, 3) if url_len else 0.0\n",
    "\n",
    "    qmarks = u.count(\"?\")\n",
    "    amps = u.count(\"&\")\n",
    "    equals = u.count(\"=\")\n",
    "    other_specials = others\n",
    "\n",
    "    U = u.upper()\n",
    "    pct_tokens = [\"%2F\", \"%3A\", \"%2E\"]\n",
    "    has_obf = 1 if (\"@\" in u or any(tok in U for tok in pct_tokens)) else 0\n",
    "    no_obf_chars = sum(U.count(tok) for tok in pct_tokens)\n",
    "    obf_ratio = round(no_obf_chars / max(1, url_len), 3)\n",
    "\n",
    "    tld_norm = \"other\" if tld in RARE_TLD else (tld or \"unknown\")\n",
    "\n",
    "    return {\n",
    "        \"URLLength\": url_len,\n",
    "        \"DomainLength\": domain_len,\n",
    "        \"TLDLength\": tld_len,\n",
    "        \"IsHTTPS\": is_https,\n",
    "        \"IsDomainIP\": is_ip,\n",
    "        \"NoOfSubDomain\": no_sub,\n",
    "        \"NoOfLettersInURL\": letters,\n",
    "        \"NoOfDegitsInURL\": digits,\n",
    "        \"DegitRatioInURL\": digit_ratio,\n",
    "        \"LetterRatioInURL\": letter_ratio,\n",
    "        \"NoOfQMarkInURL\": qmarks,\n",
    "        \"NoOfAmpersandInURL\": amps,\n",
    "        \"NoOfEqualsInURL\": equals,\n",
    "        \"NoOfOtherSpecialCharsInURL\": other_specials,\n",
    "        \"SpacialCharRatioInURL\": special_ratio,\n",
    "        \"HasObfuscation\": has_obf,\n",
    "        \"NoOfObfuscatedChar\": no_obf_chars,\n",
    "        \"ObfuscationRatio\": obf_ratio,\n",
    "        \"TLD\": tld_norm\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open(\"scripts/featureizer.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(featureizer_code)\n",
    "\n",
    "print(\"Created scripts/featureizer.py\")\n",
    "print(\"File exists:\", os.path.exists(\"scripts/featureizer.py\"))\n",
    "\n",
    "# Now import\n",
    "from scripts.featureizer import features_from_url\n",
    "print(\"Import successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c885a218-10e2-4bfc-bea9-f7ce96f8932f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- Load the schema file we saved from the notebook ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m SCHEMA_PATH = Path(\u001b[34;43m__file__\u001b[39;49m).parent / \u001b[33m\"\u001b[39m\u001b[33mmodels/schema.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(SCHEMA_PATH, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m     SCHEMA = json.load(f)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load the schema file we saved from the notebook ---\n",
    "SCHEMA_PATH = Path(__file__).parent / \"models/schema.json\"\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    SCHEMA = json.load(f)\n",
    "\n",
    "# Get the lists from the schema\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = set(SCHEMA[\"tld_categories\"])\n",
    "\n",
    "# --- Helper functions to calculate features ---\n",
    "\n",
    "def count_special_chars(url: str) -> dict:\n",
    "    \"\"\"Counts individual special characters.\"\"\"\n",
    "    counts = {\n",
    "        \"NoOfQMarkInURL\": url.count(\"?\"),\n",
    "        \"NoOfAmpersandInURL\": url.count(\"&\"),\n",
    "        \"NoOfEqualsInURL\": url.count(\"=\"),\n",
    "    }\n",
    "    \n",
    "    # For 'NoOfOtherSpecialCharsInURL', we count all non-alphanumeric chars\n",
    "    # *except* the common ones: / . : - _\n",
    "    # and the ones we already counted: ? & =\n",
    "    other_chars = re.findall(r\"[^a-zA-Z0-9/:._\\-?&=]\", url)\n",
    "    counts[\"NoOfOtherSpecialCharsInURL\"] = len(other_chars)\n",
    "    return counts\n",
    "\n",
    "def calculate_ratios(url: str, url_len: int) -> dict:\n",
    "    \"\"\"Calculates letter, digit, and special char ratios.\"\"\"\n",
    "    if url_len == 0:\n",
    "        return {\n",
    "            \"NoOfLettersInURL\": 0,\n",
    "            \"NoOfDegitsInURL\": 0,\n",
    "            \"DegitRatioInURL\": 0.0,\n",
    "            \"LetterRatioInURL\": 0.0,\n",
    "            \"SpacialCharRatioInURL\": 0.0,\n",
    "        }\n",
    "\n",
    "    letters = re.findall(r\"[a-zA-Z]\", url)\n",
    "    digits = re.findall(r\"\\d\", url)\n",
    "    special_chars = re.findall(r\"[^a-zA-Z0-9]\", url) # All non-alphanumeric\n",
    "\n",
    "    num_letters = len(letters)\n",
    "    num_digits = len(digits)\n",
    "    num_special = len(special_chars)\n",
    "\n",
    "    return {\n",
    "        \"NoOfLettersInURL\": num_letters,\n",
    "        \"NoOfDegitsInURL\": num_digits,\n",
    "        \"DegitRatioInURL\": num_digits / url_len,\n",
    "        \"LetterRatioInURL\": num_letters / url_len,\n",
    "        \"SpacialCharRatioInURL\": num_special / url_len,\n",
    "    }\n",
    "\n",
    "def check_obfuscation(url: str) -> dict:\n",
    "    \"\"\"Checks for URL-encoded chars (%XX) as a proxy for obfuscation.\"\"\"\n",
    "    # Simple check: does it contain URL encoding?\n",
    "    has_obfuscation = 1 if re.search(r\"%[0-9a-fA-F]{2}\", url) else 0\n",
    "    obfuscated_chars = re.findall(r\"%[0-9a-fA-F]{2}\", url)\n",
    "    num_obfuscated_char = len(obfuscated_chars)\n",
    "    \n",
    "    # Obfuscation ratio is the number of %-encoded chars vs. total URL length\n",
    "    ratio = num_obfuscated_char / len(url) if len(url) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"HasObfuscation\": has_obfuscation,\n",
    "        \"NoOfObfuscatedChar\": num_obfuscated_char,\n",
    "        \"ObfuscationRatio\": ratio,\n",
    "    }\n",
    "\n",
    "# --- The Main Featureizer Function ---\n",
    "\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a raw URL string and returns a single-row DataFrame\n",
    "    matching the training schema.\n",
    "    \"\"\"\n",
    "    # Ensure url has a scheme for proper parsing\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", url):\n",
    "        url = \"http://\" + url\n",
    "        \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # --- Basic Lengths ---\n",
    "        features[\"URLLength\"] = len(url)\n",
    "        \n",
    "        # --- Parse with urlparse and tldextract ---\n",
    "        parsed_url = urlparse(url)\n",
    "        tld_parts = tldextract.extract(url)\n",
    "        \n",
    "        domain = tld_parts.domain\n",
    "        subdomain = tld_parts.subdomain\n",
    "        tld = tld_parts.suffix\n",
    "        \n",
    "        features[\"Domain\"] = f\"{domain}.{tld}\"\n",
    "        features[\"DomainLength\"] = len(features[\"Domain\"])\n",
    "        \n",
    "        # --- TLD Handling (Critical!) ---\n",
    "        features[\"TLDLength\"] = len(tld)\n",
    "        # If the TLD wasn't in our training categories, map it to 'other'\n",
    "        if tld in TLD_CATEGORIES:\n",
    "            features[\"TLD\"] = tld\n",
    "        else:\n",
    "            features[\"TLD\"] = \"other\"\n",
    "            \n",
    "        # --- Protocol and IP ---\n",
    "        features[\"IsHTTPS\"] = 1 if parsed_url.scheme == \"https\" else 0\n",
    "        features[\"IsDomainIP\"] = 1 if re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\", parsed_url.netloc) else 0\n",
    "        \n",
    "        # --- Subdomain Count ---\n",
    "        if subdomain:\n",
    "            features[\"NoOfSubDomain\"] = len(subdomain.split('.'))\n",
    "        else:\n",
    "            features[\"NoOfSubDomain\"] = 0\n",
    "            \n",
    "        # --- Ratios and Obfuscation ---\n",
    "        features.update(calculate_ratios(url, features[\"URLLength\"]))\n",
    "        features.update(count_special_chars(url))\n",
    "        features.update(check_obfuscation(url))\n",
    "\n",
    "        # --- Create DataFrame ---\n",
    "        # Create a single-row DataFrame\n",
    "        df_row = pd.DataFrame([features])\n",
    "        \n",
    "        # --- Final Schema Alignment (Most Important Step) ---\n",
    "        \n",
    "        # 1. Set TLD to the correct categorical type\n",
    "        df_row[\"TLD\"] = pd.Categorical(df_row[\"TLD\"], categories=SCHEMA[\"tld_categories\"])\n",
    "        \n",
    "        # 2. Re-order all columns to *exactly* match the training order\n",
    "        df_row = df_row[FEATURE_COLS]\n",
    "        \n",
    "        return df_row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error featureizing URL {url}: {e}\")\n",
    "        # On error, return a DataFrame of NaNs/Zeros, matching the schema\n",
    "        error_df = pd.DataFrame(columns=FEATURE_COLS)\n",
    "        error_df.loc[0] = np.nan # Fill with NaN\n",
    "        # Fill TLD with the first known category to avoid dtype errors\n",
    "        error_df[\"TLD\"] = pd.Categorical(SCHEMA[\"tld_categories\"][0], categories=SCHEMA[\"tld_categories\"])\n",
    "        error_df = error_df.fillna(0) # Convert NaNs to 0\n",
    "        return error_df\n",
    "\n",
    "# --- Self-Test (to run this file directly) ---\n",
    "if __name__ == \"__main__\":\n",
    "    test_url = \"https://www.google-login.com/auth/path?user=1&session=abc%20\"\n",
    "    \n",
    "    print(f\"Testing featureizer with URL: {test_url}\")\n",
    "    features_df = featureize_url(test_url)\n",
    "    \n",
    "    print(\"\\n--- Featureizer Output ---\")\n",
    "    print(features_df.to_string())\n",
    "    \n",
    "    print(\"\\n--- Output dtypes ---\")\n",
    "    print(features_df.info())\n",
    "    \n",
    "    # Test a rare TLD\n",
    "    test_url_rare = \"http://my.bank.login.zz\" # .zz is not a real TLD\n",
    "    print(f\"\\nTesting rare TLD: {test_url_rare}\")\n",
    "    rare_df = featureize_url(test_url_rare)\n",
    "    print(f\"Mapped TLD: {rare_df['TLD'].iloc[0]}\")\n",
    "    \n",
    "    # Test an IP\n",
    "    test_url_ip = \"http://192.168.1.1/login.php\"\n",
    "    print(f\"\\nTesting IP URL: {test_url_ip}\")\n",
    "    ip_df = featureize_url(test_url_ip)\n",
    "    print(f\"IsDomainIP: {ip_df['IsDomainIP'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c00b553-a0b1-4af8-b40b-442501b3aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-friendly schema loader (no __file__)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SCHEMA_PATH = Path(\"models/schema.json\")  # relative to your notebook CWD\n",
    "if not SCHEMA_PATH.exists():\n",
    "    # If notebook is inside a subfolder, try parent\n",
    "    alt = Path(\"..\") / \"models\" / \"schema.json\"\n",
    "    if alt.exists():\n",
    "        SCHEMA_PATH = alt\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"schema.json not found at {SCHEMA_PATH} or {alt}\")\n",
    "\n",
    "with open(SCHEMA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    SCHEMA = json.load(f)\n",
    "\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = SCHEMA[\"tld_categories\"]  # keep as list; set/categorical later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991e62cc-41db-47ea-a7f6-ddb7c4aedd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the 'scripts' package is importable from the notebook\n",
    "import sys, pathlib\n",
    "\n",
    "project_root = pathlib.Path(\".\").resolve()  # adjust if notebook is inside a subfolder\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Clear any stale failed import\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k.startswith(\"scripts\"):\n",
    "        del sys.modules[k]\n",
    "\n",
    "from scripts.featureizer import features_from_url  # should work now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44bdba86-2e4a-4a75-b82e-d476dec56a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/\n",
      "  prob=0.0000 -> benign  (len=23, tld=com, subs=1, https=1)\n",
      "http://192.168.1.50/login.php?user=guest\n",
      "  prob=0.0000 -> benign  (len=40, tld=unknown, subs=0, https=0)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123\n",
      "  prob=0.0000 -> benign  (len=64, tld=co, subs=2, https=1)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\n",
      "  prob=0.0000 -> benign  (len=54, tld=com, subs=0, https=0)\n",
      "https://www.uni-mainz.de/\n",
      "  prob=0.0000 -> benign  (len=25, tld=de, subs=1, https=1)\n"
     ]
    }
   ],
   "source": [
    "# STEP N: Quick end‑to‑end sanity predictions using the calibrated model 'cal'\n",
    "import json, pandas as pd\n",
    "\n",
    "# Load schema to align columns\n",
    "SCHEMA = json.load(open(\"models/schema.json\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = SCHEMA[\"tld_categories\"]\n",
    "\n",
    "from scripts.featureizer import features_from_url  # import now that it works\n",
    "\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "\n",
    "def predict_one(u):\n",
    "    feats = features_from_url(u)\n",
    "    X = pd.DataFrame([feats])[FEATURE_COLS]\n",
    "    p = float(cal.predict_proba(X)[:,1][0])\n",
    "    label = \"phishing\" if p >= 0.5 else \"benign\"\n",
    "    return u, p, label, feats\n",
    "\n",
    "for u in tests:\n",
    "    url, p, label, f = predict_one(u)\n",
    "    print(f\"{url}\\n  prob={p:.4f} -> {label}  (len={f['URLLength']}, tld={f['TLD']}, subs={f['NoOfSubDomain']}, https={f['IsHTTPS']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdaa99af-6a95-4561-aad1-3eaace8df57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align to training schema on every inference call\n",
    "import json, pandas as pd\n",
    "\n",
    "SCHEMA = json.load(open(\"models/schema.json\", \"r\", encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = SCHEMA[\"tld_categories\"]  # ordered list\n",
    "\n",
    "def align_to_schema(df_row: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Coerce missing columns (if any) to 0\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in df_row.columns:\n",
    "            df_row[c] = 0\n",
    "    # 2) Extra columns (if any) are dropped\n",
    "    df_row = df_row[FEATURE_COLS]\n",
    "    # 3) Categorical TLD with training categories (very important)\n",
    "    df_row[\"TLD\"] = pd.Categorical(df_row[\"TLD\"], categories=TLD_CATEGORIES)\n",
    "    return df_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f105289-2c35-4b81-b5c0-8b94d6714532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/\n",
      "  prob=0.0000 -> benign  (len=23, tld=com, subs=1, https=1)\n",
      "http://192.168.1.50/login.php?user=guest\n",
      "  prob=0.0000 -> benign  (len=40, tld=unknown, subs=0, https=0)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123\n",
      "  prob=0.0000 -> benign  (len=64, tld=co, subs=2, https=1)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\n",
      "  prob=0.0000 -> benign  (len=54, tld=com, subs=0, https=0)\n",
      "https://www.uni-mainz.de/\n",
      "  prob=0.0000 -> benign  (len=25, tld=de, subs=1, https=1)\n"
     ]
    }
   ],
   "source": [
    "from scripts.featureizer import features_from_url\n",
    "\n",
    "def predict_one(u):\n",
    "    feats = features_from_url(u)                 # dict of 19 features\n",
    "    X = pd.DataFrame([feats])\n",
    "    X = align_to_schema(X)                       # enforce order and categorical TLD\n",
    "    p = float(cal.predict_proba(X)[:,1][0])\n",
    "    label = \"phishing\" if p >= 0.5 else \"benign\"\n",
    "    return u, p, label, feats\n",
    "\n",
    "for u in tests:\n",
    "    url, p, label, f = predict_one(u)\n",
    "    print(f\"{url}\\n  prob={p:.4f} -> {label}  (len={f['URLLength']}, tld={f['TLD']}, subs={f['NoOfSubDomain']}, https={f['IsHTTPS']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82426e0b-f74d-42c8-b2bd-af2185a2c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load schema + model and define a strict aligner\n",
    "import json, pandas as pd, joblib\n",
    "\n",
    "SCHEMA = json.load(open(\"models/schema.json\", \"r\", encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = SCHEMA[\"tld_categories\"]  # ordered list\n",
    "\n",
    "# Always reload the model to avoid stale state\n",
    "cal = joblib.load(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "def align_to_schema(df_row: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Add any missing columns as zeros\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in df_row.columns:\n",
    "            df_row[c] = 0\n",
    "    # Drop extras and enforce order\n",
    "    df_row = df_row[FEATURE_COLS]\n",
    "    # Enforce categorical dtype for TLD with training categories\n",
    "    df_row[\"TLD\"] = pd.Categorical(df_row[\"TLD\"], categories=TLD_CATEGORIES)\n",
    "    # Force numeric dtypes for all numerics (except TLD)\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            df_row[c] = pd.to_numeric(df_row[c], errors=\"coerce\").fillna(0)\n",
    "    return df_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47c8aab7-8ea1-4cca-b592-731a24e72374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/\n",
      "Cols match: True\n",
      "TLD dtype: category\n",
      "Unique TLD value: ['com']\n",
      "Sample row:\n",
      "  URLLength  DomainLength  IsDomainIP TLD  TLDLength  NoOfSubDomain  HasObfuscation  NoOfObfuscatedChar  ObfuscationRatio  NoOfLettersInURL  LetterRatioInURL  NoOfDegitsInURL  DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS\n",
      "        23            14           0 com          3              1               0                   0               0.0                17             0.739                0              0.0                0               0                   0                           6                  0.261        1\n",
      "prob=0.0000 -> benign\n",
      "\n",
      "http://192.168.1.50/login.php?user=guest\n",
      "Cols match: True\n",
      "TLD dtype: category\n",
      "Unique TLD value: [nan]\n",
      "Sample row:\n",
      "  URLLength  DomainLength  IsDomainIP TLD  TLDLength  NoOfSubDomain  HasObfuscation  NoOfObfuscatedChar  ObfuscationRatio  NoOfLettersInURL  LetterRatioInURL  NoOfDegitsInURL  DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS\n",
      "        40            12           1 NaN          7              0               0                   0               0.0                21             0.525                9            0.225                1               1                   0                          10                   0.25        0\n",
      "prob=0.0000 -> benign\n",
      "\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123\n",
      "Cols match: True\n",
      "TLD dtype: category\n",
      "Unique TLD value: ['co']\n",
      "Sample row:\n",
      "  URLLength  DomainLength  IsDomainIP TLD  TLDLength  NoOfSubDomain  HasObfuscation  NoOfObfuscatedChar  ObfuscationRatio  NoOfLettersInURL  LetterRatioInURL  NoOfDegitsInURL  DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS\n",
      "        64            35           0  co          2              2               0                   0               0.0                49             0.766                4            0.062                1               1                   0                          11                  0.172        1\n",
      "prob=0.0000 -> benign\n",
      "\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\n",
      "Cols match: True\n",
      "TLD dtype: category\n",
      "Unique TLD value: ['com']\n",
      "Sample row:\n",
      "  URLLength  DomainLength  IsDomainIP TLD  TLDLength  NoOfSubDomain  HasObfuscation  NoOfObfuscatedChar  ObfuscationRatio  NoOfLettersInURL  LetterRatioInURL  NoOfDegitsInURL  DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS\n",
      "        54            11           0 com          3              0               1                   3             0.056                36             0.667                3            0.056                1               1                   0                          15                  0.278        0\n",
      "prob=0.0000 -> benign\n",
      "\n",
      "https://www.uni-mainz.de/\n",
      "Cols match: True\n",
      "TLD dtype: category\n",
      "Unique TLD value: ['de']\n",
      "Sample row:\n",
      "  URLLength  DomainLength  IsDomainIP TLD  TLDLength  NoOfSubDomain  HasObfuscation  NoOfObfuscatedChar  ObfuscationRatio  NoOfLettersInURL  LetterRatioInURL  NoOfDegitsInURL  DegitRatioInURL  NoOfEqualsInURL  NoOfQMarkInURL  NoOfAmpersandInURL  NoOfOtherSpecialCharsInURL  SpacialCharRatioInURL  IsHTTPS\n",
      "        25            16           0  de          2              1               0                   0               0.0                18              0.72                0              0.0                0               0                   0                           7                   0.28        1\n",
      "prob=0.0000 -> benign\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Featureize -> align -> predict; print debug to confirm columns/dtypes\n",
    "from scripts.featureizer import features_from_url\n",
    "\n",
    "def predict_one_debug(u: str):\n",
    "    feats = features_from_url(u)                         # dict of 19 live features\n",
    "    X = pd.DataFrame([feats])\n",
    "    X = align_to_schema(X)                               # critical alignment\n",
    "    # Debug checks\n",
    "    print(\"Cols match:\", list(X.columns) == FEATURE_COLS)\n",
    "    print(\"TLD dtype:\", X[\"TLD\"].dtype)\n",
    "    print(\"Unique TLD value:\", X[\"TLD\"].unique().tolist())\n",
    "    print(\"Sample row:\\n\", X.head(1).to_string(index=False))\n",
    "    # Predict\n",
    "    p = float(cal.predict_proba(X)[:, 1][0])\n",
    "    label = \"phishing\" if p >= 0.5 else \"benign\"\n",
    "    print(f\"prob={p:.4f} -> {label}\\n\")\n",
    "    return p, label\n",
    "\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "for u in tests:\n",
    "    print(u)\n",
    "    predict_one_debug(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34c4d4d6-733e-41e5-bb08-9d9d10984644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/ \n",
      " prob= 0.0 -> benign | TLD: com | IsHTTPS: 1 | len: 23\n",
      "http://192.168.1.50/login.php?user=guest \n",
      " prob= 0.0 -> benign | TLD: other | IsHTTPS: 0 | len: 40\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 \n",
      " prob= 0.0 -> benign | TLD: co | IsHTTPS: 1 | len: 64\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru \n",
      " prob= 0.0 -> benign | TLD: com | IsHTTPS: 0 | len: 54\n",
      "https://www.uni-mainz.de/ \n",
      " prob= 0.0 -> benign | TLD: de | IsHTTPS: 1 | len: 25\n"
     ]
    }
   ],
   "source": [
    "# Patch: robust schema alignment handling NaN/unknown TLD and numeric types\n",
    "import json, pandas as pd, joblib\n",
    "from scripts.featureizer import features_from_url\n",
    "\n",
    "SCHEMA = json.load(open(\"models/schema.json\", \"r\", encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATEGORIES = SCHEMA[\"tld_categories\"]  # must include \"other\"\n",
    "assert \"TLD\" in FEATURE_COLS, \"TLD must be part of features\"\n",
    "assert \"other\" in TLD_CATEGORIES, \"'other' must be in training categories\"\n",
    "\n",
    "cal = joblib.load(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "def align_to_schema_strict(feats: dict) -> pd.DataFrame:\n",
    "    X = pd.DataFrame([feats])\n",
    "    # 1) Ensure TLD exists; map empty/NaN to 'other'\n",
    "    if (\"TLD\" not in X.columns) or (pd.isna(X.at[0, \"TLD\"])) or (str(X.at[0, \"TLD\"]).strip() == \"\"):\n",
    "        X[\"TLD\"] = \"other\"\n",
    "    else:\n",
    "        t = str(X.at[0, \"TLD\"])\n",
    "        X.at[0, \"TLD\"] = t if t in TLD_CATEGORIES else \"other\"\n",
    "    # 2) Add missing columns as zeros\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    # 3) Drop extra cols, enforce order\n",
    "    X = X[FEATURE_COLS]\n",
    "    # 4) Cast numerics and TLD categorical\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATEGORIES)\n",
    "    return X\n",
    "\n",
    "def predict_one_fixed(u: str):\n",
    "    feats = features_from_url(u)\n",
    "    X = align_to_schema_strict(feats)\n",
    "    p = float(cal.predict_proba(X)[:, 1][0])\n",
    "    label = \"phishing\" if p >= 0.5 else \"benign\"\n",
    "    return p, label, X\n",
    "\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "for u in tests:\n",
    "    p, label, X = predict_one_fixed(u)\n",
    "    print(u, \"\\n prob=\", round(p, 4), \"->\", label, \"| TLD:\", X.at[0,\"TLD\"], \"| IsHTTPS:\", X.at[0,\"IsHTTPS\"], \"| len:\", X.at[0,\"URLLength\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66e6118e-0765-4378-9b3d-109a5b5842bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_COLS (len): 19\n",
      "['URLLength', 'DomainLength', 'IsDomainIP', 'TLD', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS']\n",
      "TLD categories sample: ['ae', 'ai', 'am', 'app', 'ar', 'art', 'asia', 'at', 'au', 'az']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "SCHEMA = json.load(open(\"models/schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "print(\"FEATURE_COLS (len):\", len(SCHEMA[\"feature_cols\"]))\n",
    "print(SCHEMA[\"feature_cols\"])\n",
    "print(\"TLD categories sample:\", SCHEMA[\"tld_categories\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8679f70-31c7-4a0b-8bd9-9d4d4fe02dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, joblib\n",
    "\n",
    "SCHEMA = json.load(open(\"models/schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]  # ordered list\n",
    "cal = joblib.load(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "def align_to_schema_strict(feats: dict) -> pd.DataFrame:\n",
    "    X = pd.DataFrame([feats])\n",
    "    # Normalize TLD\n",
    "    if \"TLD\" not in X or pd.isna(X.at[0,\"TLD\"]) or str(X.at[0,\"TLD\"]).strip()==\"\":\n",
    "        X[\"TLD\"] = \"other\"\n",
    "    else:\n",
    "        t = str(X.at[0,\"TLD\"]).lower()\n",
    "        X.at[0,\"TLD\"] = t if t in TLD_CATS else \"other\"\n",
    "    # Ensure all expected cols exist; extras dropped\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    # Dtypes\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5968804c-8fca-486b-bf7a-a70df1a12971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val proba sample: [0.9969, 0.9959, 0.9983, 0.996, 0.9978]\n"
     ]
    }
   ],
   "source": [
    "# Should print non-trivial values if the model is healthy\n",
    "import numpy as np\n",
    "print(\"val proba sample:\", np.round(cal.predict_proba(X_val[:5])[:,1], 4).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91686f54-0675-4974-8a42-55aa1e0bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e4ba27-bf04-4369-9176-055a131cef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((188636, 19), (47159, 19))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP A1: Recreate X,y strictly from the 19 live-computable columns in your schema\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"PhiUSIIL_Phishing_URL_Dataset.csv\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"URLLength\",\"DomainLength\",\"IsDomainIP\",\"TLD\",\"TLDLength\",\n",
    "    \"NoOfSubDomain\",\"HasObfuscation\",\"NoOfObfuscatedChar\",\"ObfuscationRatio\",\n",
    "    \"NoOfLettersInURL\",\"LetterRatioInURL\",\"NoOfDegitsInURL\",\"DegitRatioInURL\",\n",
    "    \"NoOfEqualsInURL\",\"NoOfQMarkInURL\",\"NoOfAmpersandInURL\",\n",
    "    \"NoOfOtherSpecialCharsInURL\",\"SpacialCharRatioInURL\",\"IsHTTPS\"\n",
    "]\n",
    "\n",
    "use_cols = [\"URL\",\"Domain\"] + feature_cols + [\"label\"]\n",
    "df = pd.read_csv(DATA_PATH, usecols=use_cols).dropna()\n",
    "\n",
    "# Rare TLD collapse: EXACTLY as used for training previously (threshold 50)\n",
    "tld_counts = df[\"TLD\"].fillna(\"unknown\").str.lower().value_counts()\n",
    "rare = set(tld_counts[tld_counts < 50].index)\n",
    "df[\"TLD\"] = df[\"TLD\"].fillna(\"unknown\").str.lower().apply(lambda x: \"other\" if x in rare else x)\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "# TLD categorical with stable order (sorted ensures determinism)\n",
    "tld_order = sorted(X[\"TLD\"].astype(str).unique().tolist())\n",
    "if \"other\" not in tld_order:\n",
    "    tld_order = [\"other\"] + [t for t in tld_order if t != \"other\"]\n",
    "X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=tld_order)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c91971ec-9adf-4421-aaeb-ca92f80d7e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python313\\Lib\\site-packages\\sklearn\\calibration.py:337: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': np.float64(0.9989957719351902), 'AP': np.float64(0.9988523965504068), 'F1@0.5': 0.9978532035385128, 'Prec@90%Rec': 0.5718950783519583, 'Thr@90%Rec': 0.9999865414941713}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "\n",
    "categorical_features = [X_train.columns.get_loc(\"TLD\")]\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_leaf_nodes=31,\n",
    "    learning_rate=0.08,\n",
    "    max_iter=300,\n",
    "    validation_fraction=0.1,\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    categorical_features=categorical_features\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "cal = CalibratedClassifierCV(hgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "proba = cal.predict_proba(X_val)[:,1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val, proba)\n",
    "ap = average_precision_score(y_val, proba)\n",
    "f1 = f1_score(y_val, pred)\n",
    "\n",
    "prec, rec, th = precision_recall_curve(y_val, proba)\n",
    "target_recall = 0.90\n",
    "idx = np.where(rec >= target_recall)[0]\n",
    "p_at_90 = float(prec[idx[0]]) if len(idx) else float(\"nan\")\n",
    "thr_at_90 = float(th[idx[0]-1]) if len(idx) else float(\"nan\")\n",
    "\n",
    "print({\"AUC\":auc, \"AP\":ap, \"F1@0.5\":f1, \"Prec@90%Rec\":p_at_90, \"Thr@90%Rec\":thr_at_90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35f8f3d9-207a-4028-a220-aa0ef825317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model + schema for 19 live features\n"
     ]
    }
   ],
   "source": [
    "import json, joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(cal, \"models/phish_url_hgb_cal.joblib\")\n",
    "SCHEMA = {\n",
    "    \"feature_cols\": list(X_train.columns),\n",
    "    \"tld_categories\": list(X_train[\"TLD\"].cat.categories)\n",
    "}\n",
    "json.dump(SCHEMA, open(\"models/schema.json\",\"w\"), ensure_ascii=False)\n",
    "print(\"Saved model + schema for 19 live features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ded153c1-0c48-4797-867d-42f9a6eaf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "\n",
    "SCHEMA = json.load(open(\"models/schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    u = (url or \"\").strip()\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", u):\n",
    "        u = \"http://\" + u\n",
    "    parsed = urlparse(u)\n",
    "    ext = tldextract.extract(u)\n",
    "    tld = (ext.suffix or \"unknown\").lower()\n",
    "\n",
    "    url_len = len(u)\n",
    "    domain = \".\".join([p for p in [ext.subdomain, ext.domain, ext.suffix] if p])\n",
    "    domain_len = len(domain)\n",
    "\n",
    "    is_https = 1 if (parsed.scheme or \"\").lower()==\"https\" else 0\n",
    "    is_ip = 1 if re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", ext.domain or \"\") else 0\n",
    "    subdomain = ext.subdomain or \"\"\n",
    "    no_sub = 0 if not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "    letters = len(re.findall(r\"[A-Za-z]\", u))\n",
    "    digits  = len(re.findall(r\"[0-9]\", u))\n",
    "    others  = len(re.findall(r\"[^A-Za-z0-9]\", u))\n",
    "\n",
    "    letter_ratio = (letters/url_len) if url_len else 0.0\n",
    "    digit_ratio  = (digits/url_len)  if url_len else 0.0\n",
    "    special_ratio= (others/url_len)  if url_len else 0.0\n",
    "\n",
    "    qmarks = u.count(\"?\")\n",
    "    amps   = u.count(\"&\")\n",
    "    equals = u.count(\"=\")\n",
    "\n",
    "    U = u.upper()\n",
    "    pct_tokens = [\"%2F\",\"%3A\",\"%2E\"]\n",
    "    has_obf = 1 if (\"@\" in u or any(tok in U for tok in pct_tokens)) else 0\n",
    "    no_obf = sum(U.count(tok) for tok in pct_tokens)\n",
    "    obf_ratio = (no_obf/url_len) if url_len else 0.0\n",
    "\n",
    "    tld_norm = tld if tld in TLD_CATS else \"other\"\n",
    "\n",
    "    row = {\n",
    "        \"URLLength\": url_len,\n",
    "        \"DomainLength\": domain_len,\n",
    "        \"IsDomainIP\": is_ip,\n",
    "        \"TLD\": tld_norm,\n",
    "        \"TLDLength\": len(tld),\n",
    "        \"NoOfSubDomain\": no_sub,\n",
    "        \"HasObfuscation\": has_obf,\n",
    "        \"NoOfObfuscatedChar\": no_obf,\n",
    "        \"ObfuscationRatio\": obf_ratio,\n",
    "        \"NoOfLettersInURL\": letters,\n",
    "        \"LetterRatioInURL\": letter_ratio,\n",
    "        \"NoOfDegitsInURL\": digits,\n",
    "        \"DegitRatioInURL\": digit_ratio,\n",
    "        \"NoOfEqualsInURL\": equals,\n",
    "        \"NoOfQMarkInURL\": qmarks,\n",
    "        \"NoOfAmpersandInURL\": amps,\n",
    "        \"NoOfOtherSpecialCharsInURL\": others,\n",
    "        \"SpacialCharRatioInURL\": special_ratio,\n",
    "        \"IsHTTPS\": is_https\n",
    "    }\n",
    "    X = pd.DataFrame([row])\n",
    "    # enforce order and dtypes\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c]=0\n",
    "    X = X[FEATURE_COLS]\n",
    "    for c in FEATURE_COLS:\n",
    "        if c!=\"TLD\":\n",
    "            X[c]=pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8467967-ac88-40c8-be04-7c073654786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and schema.json loaded successfully.\n",
      "\n",
      "--- Running Tests ---\n",
      "Error featureizing URL https://www.google.com/: Cannot setitem on a Categorical with a new category (0), set the categories first\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Categorical input must be list-like",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mfeatureize_url\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m    132\u001b[39m df_row = df_row[FEATURE_COLS]\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m df_row = \u001b[43mdf_row\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Final safety net\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_row\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\generic.py:7453\u001b[39m, in \u001b[36mNDFrame.fillna\u001b[39m\u001b[34m(self, value, method, axis, inplace, limit, downcast)\u001b[39m\n\u001b[32m   7452\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7453\u001b[39m         new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\n\u001b[32m   7455\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7456\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\internals\\base.py:186\u001b[39m, in \u001b[36mDataManager.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast)\u001b[39m\n\u001b[32m    184\u001b[39m     limit = libalgos.validate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit=limit)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfillna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m result_blocks = extend_blocks(applied, result_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2407\u001b[39m, in \u001b[36mExtensionBlock.fillna\u001b[39m\u001b[34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[39m\n\u001b[32m   2406\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2408\u001b[39m \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[32m   2409\u001b[39m \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:376\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.fillna\u001b[39m\u001b[34m(self, value, method, limit, copy)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1590\u001b[39m, in \u001b[36mCategorical._validate_setitem_value\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1615\u001b[39m, in \u001b[36mCategorical._validate_scalar\u001b[39m\u001b[34m(self, fill_value)\u001b[39m\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1616\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot setitem on a Categorical with a new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1617\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), set the categories first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1618\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[31mTypeError\u001b[39m: Cannot setitem on a Categorical with a new category (0), set the categories first",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    148\u001b[39m tests = [\n\u001b[32m    149\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://www.google.com/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttp://192.168.1.50/login.php?user=guest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://www.uni-mainz.de/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m ]\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m tests:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     Xinf = \u001b[43mfeatureize_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     p = \u001b[38;5;28mfloat\u001b[39m(cal.predict_proba(Xinf)[:,\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mprint\u001b[39m(u, \u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(p, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mfeatureize_url\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m    139\u001b[39m error_df = pd.DataFrame(columns=FEATURE_COLS)\n\u001b[32m    140\u001b[39m error_df.loc[\u001b[32m0\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m error_df[\u001b[33m\"\u001b[39m\u001b[33mTLD\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mother\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCHEMA\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtld_categories\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_df.fillna(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:406\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(values):\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# GH#38433\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCategorical input must be list-like\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# null_mask indicates missing values we want to exclude from inference.\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# This means: only missing values in list-likes (not arrays/ndframes).\u001b[39;00m\n\u001b[32m    410\u001b[39m null_mask = np.array(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: Categorical input must be list-like"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Load Schema and Model (Fixed for Notebook) ---\n",
    "# This path works from your notebook, assuming 'models' folder is in the same directory\n",
    "SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "MODEL_PATH = Path(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "try:\n",
    "    with open(SCHEMA_PATH, \"r\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    \n",
    "    # Load the model\n",
    "    cal = joblib.load(MODEL_PATH)\n",
    "    \n",
    "    # Get lists from the schema\n",
    "    FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "    TLD_CATEGORIES = set(SCHEMA[\"tld_categories\"])\n",
    "    \n",
    "    print(\"Model and schema.json loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not find '{SCHEMA_PATH}' or '{MODEL_PATH}'.\")\n",
    "    print(\"Please make sure these files are in a 'models' folder in the same directory as your notebook.\")\n",
    "    # Create dummy vars so the rest of the cell can run\n",
    "    FEATURE_COLS = []\n",
    "    TLD_CATEGORIES = set()\n",
    "    cal = None\n",
    "\n",
    "# --- 2. Helper Functions (Required by the Featureizer) ---\n",
    "\n",
    "def count_special_chars(url: str) -> dict:\n",
    "    counts = {\n",
    "        \"NoOfQMarkInURL\": url.count(\"?\"),\n",
    "        \"NoOfAmpersandInURL\": url.count(\"&\"),\n",
    "        \"NoOfEqualsInURL\": url.count(\"=\"),\n",
    "    }\n",
    "    other_chars = re.findall(r\"[^a-zA-Z0-9/:._\\-?&=]\", url)\n",
    "    counts[\"NoOfOtherSpecialCharsInURL\"] = len(other_chars)\n",
    "    return counts\n",
    "\n",
    "def calculate_ratios(url: str, url_len: int) -> dict:\n",
    "    if url_len == 0:\n",
    "        return {\"NoOfLettersInURL\": 0, \"NoOfDegitsInURL\": 0, \"DegitRatioInURL\": 0.0, \"LetterRatioInURL\": 0.0, \"SpacialCharRatioInURL\": 0.0}\n",
    "\n",
    "    letters = re.findall(r\"[a-zA-Z]\", url)\n",
    "    digits = re.findall(r\"\\d\", url)\n",
    "    special_chars = re.findall(r\"[^a-zA-Z0-9]\", url)\n",
    "    num_letters, num_digits, num_special = len(letters), len(digits), len(special_chars)\n",
    "\n",
    "    return {\n",
    "        \"NoOfLettersInURL\": num_letters,\n",
    "        \"NoOfDegitsInURL\": num_digits,\n",
    "        \"DegitRatioInURL\": num_digits / url_len if url_len > 0 else 0.0,\n",
    "        \"LetterRatioInURL\": num_letters / url_len if url_len > 0 else 0.0,\n",
    "        \"SpacialCharRatioInURL\": num_special / url_len if url_len > 0 else 0.0,\n",
    "    }\n",
    "\n",
    "def check_obfuscation(url: str) -> dict:\n",
    "    has_obfuscation = 1 if re.search(r\"%[0-9a-fA-F]{2}\", url) else 0\n",
    "    obfuscated_chars = re.findall(r\"%[0-9a-fA-F]{2}\", url)\n",
    "    num_obfuscated_char = len(obfuscated_chars)\n",
    "    ratio = num_obfuscated_char / len(url) if len(url) > 0 else 0.0\n",
    "    return {\"HasObfuscation\": has_obfuscation, \"NoOfObfuscatedChar\": num_obfuscated_char, \"ObfuscationRatio\": ratio}\n",
    "\n",
    "# --- 3. The Corrected featureize_url Function ---\n",
    "# (This definition will override any old ones)\n",
    "\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a raw URL string and returns a single-row DataFrame\n",
    "    matching the training schema.\n",
    "    \"\"\"\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", url):\n",
    "        url = \"http://\" + url\n",
    "        \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        features[\"URLLength\"] = len(url)\n",
    "        \n",
    "        parsed_url = urlparse(url)\n",
    "        tld_parts = tldextract.extract(url)\n",
    "        \n",
    "        # FIX: Use netloc (hostname) for 'Domain' to match CSV\n",
    "        hostname = parsed_url.netloc\n",
    "        if not hostname: # Fallback for simple strings like 'badsite.com'\n",
    "             hostname = tld_parts.subdomain + \".\" + tld_parts.domain + \".\" + tld_parts.suffix\n",
    "             hostname = hostname.strip('.')\n",
    "        \n",
    "        features[\"DomainLength\"] = len(hostname)\n",
    "        \n",
    "        tld = tld_parts.suffix\n",
    "        subdomain_str = tld_parts.subdomain\n",
    "        \n",
    "        # FIX: Check for IP Address\n",
    "        is_ip = 1 if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", tld_parts.domain) and tld_parts.suffix == '' else 0\n",
    "        features[\"IsDomainIP\"] = is_ip\n",
    "        \n",
    "        if is_ip:\n",
    "            # Logic for IPs\n",
    "            features[\"TLD\"] = \"other\"  # Map IPs to 'other' as they have no TLD\n",
    "            features[\"TLDLength\"] = 0\n",
    "            features[\"NoOfSubDomain\"] = 0\n",
    "        else:\n",
    "            # Logic for normal domains\n",
    "            features[\"TLDLength\"] = len(tld)\n",
    "            features[\"TLD\"] = \"other\" if tld not in TLD_CATEGORIES else tld\n",
    "            features[\"NoOfSubDomain\"] = len(subdomain_str.split('.')) if subdomain_str else 0\n",
    "        \n",
    "        features[\"IsHTTPS\"] = 1 if parsed_url.scheme == \"https\" else 0\n",
    "        \n",
    "        features.update(calculate_ratios(url, features[\"URLLength\"]))\n",
    "        features.update(count_special_chars(url))\n",
    "        features.update(check_obfuscation(url))\n",
    "\n",
    "        df_row = pd.DataFrame([features])\n",
    "        \n",
    "        # --- Final Schema Alignment ---\n",
    "        df_row[\"TLD\"] = pd.Categorical(df_row[\"TLD\"], categories=SCHEMA[\"tld_categories\"])\n",
    "        \n",
    "        for col in FEATURE_COLS:\n",
    "            if col not in df_row.columns:\n",
    "                df_row[col] = 0\n",
    "                \n",
    "        df_row = df_row[FEATURE_COLS]\n",
    "        df_row = df_row.fillna(0) # Final safety net\n",
    "        \n",
    "        return df_row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error featureizing URL {url}: {e}\")\n",
    "        error_df = pd.DataFrame(columns=FEATURE_COLS)\n",
    "        error_df.loc[0] = 0\n",
    "        error_df[\"TLD\"] = pd.Categorical(\"other\", categories=SCHEMA[\"tld_categories\"])\n",
    "        return error_df.fillna(0)\n",
    "\n",
    "# --- 4. Your Test Script (Now using the function defined above) ---\n",
    "\n",
    "if cal:\n",
    "    print(\"\\n--- Running Tests ---\")\n",
    "    tests = [\n",
    "        \"https://www.google.com/\",\n",
    "        \"http://192.168.1.50/login.php?user=guest\",\n",
    "        \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Phishing\n",
    "        \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\", # Obfuscation\n",
    "        \"https://www.uni-mainz.de/\",\n",
    "    ]\n",
    "    for u in tests:\n",
    "        Xinf = featureize_url(u)\n",
    "        p = float(cal.predict_proba(Xinf)[:,1][0])\n",
    "        print(u, \"->\", round(p, 4))\n",
    "else:\n",
    "    print(\"\\nTests skipped because model could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "903dbf7f-58c2-46e2-be4d-c95e5375670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, re, joblib\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "# Load schema/model\n",
    "SCHEMA = json.load(open(\"models/schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]  # ordered, should include \"other\"\n",
    "cal = joblib.load(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "def _count_other_specials(u: str) -> int:\n",
    "    # Match training: count all non-alphanumeric as \"other specials\"\n",
    "    return len(re.findall(r\"[^A-Za-z0-9]\", u))\n",
    "\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    u = (url or \"\").strip()\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", u):\n",
    "        u = \"http://\" + u\n",
    "\n",
    "    parsed = urlparse(u)\n",
    "    ext = tldextract.extract(u)\n",
    "    tld_raw = (ext.suffix or \"unknown\").lower()\n",
    "\n",
    "    url_len = len(u)\n",
    "    domain = \".\".join([p for p in [ext.subdomain, ext.domain, ext.suffix] if p])\n",
    "    domain_len = len(domain)\n",
    "\n",
    "    is_https = 1 if (parsed.scheme or \"\").lower() == \"https\" else 0\n",
    "    is_ip = 1 if re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", ext.domain or \"\") else 0\n",
    "    subdomain = ext.subdomain or \"\"\n",
    "    no_sub = 0 if not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "    letters = len(re.findall(r\"[A-Za-z]\", u))\n",
    "    digits  = len(re.findall(r\"[0-9]\", u))\n",
    "    others  = _count_other_specials(u)\n",
    "\n",
    "    letter_ratio  = (letters/url_len) if url_len else 0.0\n",
    "    digit_ratio   = (digits/url_len)  if url_len else 0.0\n",
    "    special_ratio = (others/url_len)  if url_len else 0.0\n",
    "\n",
    "    qmarks = u.count(\"?\")\n",
    "    amps   = u.count(\"&\")\n",
    "    equals = u.count(\"=\")\n",
    "\n",
    "    U = u.upper()\n",
    "    pct_tokens = [\"%2F\",\"%3A\",\"%2E\"]\n",
    "    has_obf = 1 if (\"@\" in u or any(tok in U for tok in pct_tokens)) else 0\n",
    "    no_obf = sum(U.count(tok) for tok in pct_tokens)\n",
    "    obf_ratio = (no_obf/url_len) if url_len else 0.0\n",
    "\n",
    "    # Map TLD into training categories\n",
    "    tld_norm = tld_raw if tld_raw in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "\n",
    "    row = {\n",
    "        \"URLLength\": url_len,\n",
    "        \"DomainLength\": domain_len,\n",
    "        \"IsDomainIP\": is_ip,\n",
    "        \"TLD\": tld_norm,\n",
    "        \"TLDLength\": len(tld_raw),\n",
    "        \"NoOfSubDomain\": no_sub,\n",
    "        \"HasObfuscation\": has_obf,\n",
    "        \"NoOfObfuscatedChar\": no_obf,\n",
    "        \"ObfuscationRatio\": obf_ratio,\n",
    "        \"NoOfLettersInURL\": letters,\n",
    "        \"LetterRatioInURL\": letter_ratio,\n",
    "        \"NoOfDegitsInURL\": digits,\n",
    "        \"DegitRatioInURL\": digit_ratio,\n",
    "        \"NoOfEqualsInURL\": equals,\n",
    "        \"NoOfQMarkInURL\": qmarks,\n",
    "        \"NoOfAmpersandInURL\": amps,\n",
    "        \"NoOfOtherSpecialCharsInURL\": others,\n",
    "        \"SpacialCharRatioInURL\": special_ratio,\n",
    "        \"IsHTTPS\": is_https\n",
    "    }\n",
    "\n",
    "    X = pd.DataFrame([row])\n",
    "    # Ensure all expected columns exist\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    # Reorder\n",
    "    X = X[FEATURE_COLS]\n",
    "\n",
    "    # Cast numerics and set categorical LAST (avoid fillna on categorical)\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5d00ba8-1f36-470b-913f-1f2908230701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python313\\Lib\\site-packages\\sklearn\\calibration.py:337: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': np.float64(0.9987836303990693), 'AP': np.float64(0.9986261356966399), 'F1@0.5': 0.9970950134147469, 'Prec@90%Rec': 0.5718950783519583, 'Thr@90%Rec': 0.9999994336470258}\n"
     ]
    }
   ],
   "source": [
    "# Rebuild X directly from URLs for perfect parity\n",
    "urls = df[\"URL\"].tolist()\n",
    "rows = []\n",
    "for u in urls:\n",
    "    rows.append(featureize_url(u).iloc[0].to_dict())\n",
    "X2 = pd.DataFrame(rows)[FEATURE_COLS]\n",
    "y2 = df[\"label\"].astype(int)\n",
    "\n",
    "# Ensure TLD categorical matches and numerics are numeric\n",
    "for c in FEATURE_COLS:\n",
    "    if c != \"TLD\":\n",
    "        X2[c] = pd.to_numeric(X2[c], errors=\"coerce\").fillna(0)\n",
    "X2[\"TLD\"] = pd.Categorical(X2[\"TLD\"], categories=TLD_CATS)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr, Xva, ytr, yva = train_test_split(X2, y2, test_size=0.20, stratify=y2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "import numpy as np, joblib, json, os\n",
    "\n",
    "categorical_features = [Xtr.columns.get_loc(\"TLD\")]\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_leaf_nodes=31, learning_rate=0.08, max_iter=300,\n",
    "    validation_fraction=0.1, early_stopping=True, random_state=42,\n",
    "    categorical_features=categorical_features\n",
    ")\n",
    "hgb.fit(Xtr, ytr)\n",
    "cal2 = CalibratedClassifierCV(hgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "cal2.fit(Xva, yva)\n",
    "\n",
    "proba2 = cal2.predict_proba(Xva)[:,1]\n",
    "pred2 = (proba2 >= 0.5).astype(int)\n",
    "auc2 = roc_auc_score(yva, proba2)\n",
    "ap2 = average_precision_score(yva, proba2)\n",
    "f12 = f1_score(yva, pred2)\n",
    "prec, rec, th = precision_recall_curve(yva, proba2)\n",
    "idx = np.where(rec >= 0.90)[0]\n",
    "p_at_90 = float(prec[idx[0]]) if len(idx) else float(\"nan\")\n",
    "thr_at_90 = float(th[idx[0]-1]) if len(idx) else float(\"nan\")\n",
    "print({\"AUC\":auc2, \"AP\":ap2, \"F1@0.5\":f12, \"Prec@90%Rec\":p_at_90, \"Thr@90%Rec\":thr_at_90})\n",
    "\n",
    "# Save parity-locked model and schema\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(cal2, \"models/phish_url_hgb_cal.joblib\")\n",
    "SCHEMA2 = {\"feature_cols\": FEATURE_COLS, \"tld_categories\": TLD_CATS}\n",
    "json.dump(SCHEMA2, open(\"models/schema.json\",\"w\"), ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2017c519-f6ee-4b39-aaa4-5beb50bc5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained model and schema loaded successfully.\n",
      "\n",
      "--- Running Tests with Retrained Model ---\n",
      "https://www.google.com/ -> 0.1087 (benign)\n",
      "http://192.168.1.50/login.php?user=guest -> 0.0 (benign)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 -> 0.0 (benign)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru -> 0.0 (benign)\n",
      "https://www.uni-mainz.de/ -> 0.9448 (PHISHING)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load the NEWLY SAVED Schema and Model ---\n",
    "SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "MODEL_PATH = Path(\"models/phish_url_hgb_cal.joblib\") # This should now point to the retrained model\n",
    "\n",
    "try:\n",
    "    with open(SCHEMA_PATH, \"r\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    cal = joblib.load(MODEL_PATH) # Load the RETRAINED model\n",
    "    FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "    TLD_CATEGORIES = SCHEMA[\"tld_categories\"]\n",
    "    print(\"Retrained model and schema loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not find '{SCHEMA_PATH}' or '{MODEL_PATH}'.\")\n",
    "    FEATURE_COLS = []\n",
    "    TLD_CATEGORIES = []\n",
    "    cal = None\n",
    "\n",
    "# --- Use the SAME featureize_url function ---\n",
    "# (Make sure the definition from the retraining cell is still active in your notebook's memory,\n",
    "# or redefine it here if necessary)\n",
    "# Define featureize_url and helpers (count_special_chars, calculate_ratios, check_obfuscation)\n",
    "# ... [Paste the entire corrected featureize_url function definition here again if needed] ...\n",
    "\n",
    "# --- Run the Tests ---\n",
    "if cal:\n",
    "    print(\"\\n--- Running Tests with Retrained Model ---\")\n",
    "    tests = [\n",
    "        \"https://www.google.com/\",\n",
    "        \"http://192.168.1.50/login.php?user=guest\",\n",
    "        \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Phishing\n",
    "        \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\", # Obfuscation/Phishing\n",
    "        \"https://www.uni-mainz.de/\",\n",
    "    ]\n",
    "    for u in tests:\n",
    "        Xinf = featureize_url(u)\n",
    "        if not Xinf.empty and cal:\n",
    "            p = float(cal.predict_proba(Xinf)[:,1][0])\n",
    "            label = \"PHISHING\" if p > 0.5 else \"benign\"\n",
    "            print(f\"{u} -> {round(p, 4)} ({label})\")\n",
    "        else:\n",
    "            print(f\"Featureization failed or model not loaded for {u}, skipping prediction.\")\n",
    "else:\n",
    "    print(\"\\nTests skipped because model could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b1932e6-712a-4d36-8e94-0fcc30ce2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re, pandas as pd, tldextract\n",
    "\n",
    "def _is_ipv4(host: str) -> bool:\n",
    "    if not host: return False\n",
    "    m = re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", host)\n",
    "    if not m: return False\n",
    "    return all(0 <= int(p) <= 255 for p in host.split(\".\"))\n",
    "\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    u = (url or \"\").strip()\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", u):\n",
    "        u = \"http://\" + u\n",
    "\n",
    "    parsed = urlparse(u)\n",
    "    host = parsed.hostname or \"\"  # robust hostname for IP/TLD checks\n",
    "    ext = tldextract.extract(u)\n",
    "    tld_raw = (ext.suffix or \"unknown\").lower()\n",
    "\n",
    "    url_len = len(u)\n",
    "    domain = \".\".join([p for p in [ext.subdomain, ext.domain, ext.suffix] if p])\n",
    "    domain_len = len(domain)\n",
    "\n",
    "    is_https = 1 if (parsed.scheme or \"\").lower() == \"https\" else 0\n",
    "    is_ip = 1 if _is_ipv4(host) else 0  # FIX: IP from hostname\n",
    "    subdomain = ext.subdomain or \"\"\n",
    "    no_sub = 0 if not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "    letters = len(re.findall(r\"[A-Za-z]\", u))\n",
    "    digits  = len(re.findall(r\"[0-9]\", u))\n",
    "    others  = len(re.findall(r\"[^A-Za-z0-9]\", u))  # match training rule\n",
    "\n",
    "    letter_ratio  = (letters/url_len) if url_len else 0.0\n",
    "    digit_ratio   = (digits/url_len)  if url_len else 0.0\n",
    "    special_ratio = (others/url_len)  if url_len else 0.0\n",
    "\n",
    "    qmarks = u.count(\"?\")\n",
    "    amps   = u.count(\"&\")\n",
    "    equals = u.count(\"=\")\n",
    "\n",
    "    U = u.upper()\n",
    "    pct_tokens = [\"%2F\",\"%3A\",\"%2E\"]\n",
    "    has_obf = 1 if (\"@\" in u or any(tok in U for tok in pct_tokens)) else 0\n",
    "    no_obf = sum(U.count(tok) for tok in pct_tokens)\n",
    "    obf_ratio = (no_obf/url_len) if url_len else 0.0\n",
    "\n",
    "    # TLD mapping strictly into training categories\n",
    "    tld_norm = tld_raw if tld_raw in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "\n",
    "    row = {\n",
    "        \"URLLength\": url_len, \"DomainLength\": domain_len, \"IsDomainIP\": is_ip,\n",
    "        \"TLD\": tld_norm, \"TLDLength\": len(tld_raw),\n",
    "        \"NoOfSubDomain\": no_sub,\n",
    "        \"HasObfuscation\": has_obf, \"NoOfObfuscatedChar\": no_obf, \"ObfuscationRatio\": obf_ratio,\n",
    "        \"NoOfLettersInURL\": letters, \"LetterRatioInURL\": letter_ratio,\n",
    "        \"NoOfDegitsInURL\": digits, \"DegitRatioInURL\": digit_ratio,\n",
    "        \"NoOfEqualsInURL\": equals, \"NoOfQMarkInURL\": qmarks, \"NoOfAmpersandInURL\": amps,\n",
    "        \"NoOfOtherSpecialCharsInURL\": others, \"SpacialCharRatioInURL\": special_ratio,\n",
    "        \"IsHTTPS\": is_https\n",
    "    }\n",
    "    X = pd.DataFrame([row])\n",
    "    # enforce order & dtypes (numerics first, then categorical)\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8361dba9-cdfb-4322-8bf7-c9cbc0de2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema/dtype check: OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sampa\\AppData\\Local\\Temp\\ipykernel_19068\\3594749266.py:18: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  assert pd.api.types.is_categorical_dtype(X[\"TLD\"]), \"TLD is not categorical dtype\"\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd, numpy as np\n",
    "\n",
    "# Load schema\n",
    "SCHEMA = json.load(open(\"models/schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "\n",
    "# 1) Make a dummy URL and build features\n",
    "X = featureize_url(\"https://example.com/test?x=1&y=2\")\n",
    "\n",
    "# 2) Column set and order\n",
    "assert list(X.columns) == FEATURE_COLS, f\"Column order mismatch:\\n{list(X.columns)}\\n!=\\n{FEATURE_COLS}\"\n",
    "\n",
    "# 3) Dtypes: all non-TLD numeric, TLD is category with expected categories\n",
    "for c in FEATURE_COLS:\n",
    "    if c != \"TLD\":\n",
    "        assert pd.api.types.is_numeric_dtype(X[c]), f\"{c} is not numeric dtype\"\n",
    "assert pd.api.types.is_categorical_dtype(X[\"TLD\"]), \"TLD is not categorical dtype\"\n",
    "cats = list(X[\"TLD\"].cat.categories)\n",
    "assert cats == TLD_CATS, f\"TLD category space mismatch:\\n{cats[:10]} ...\"\n",
    "\n",
    "# 4) No NaNs in numerics\n",
    "num_cols = [c for c in FEATURE_COLS if c != \"TLD\"]\n",
    "assert np.isfinite(X[num_cols].to_numpy()).all(), \"Found NaN/inf in numeric features\"\n",
    "\n",
    "print(\"Schema/dtype check: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "369367cc-abaa-4d97-a73d-d04815bc8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/ -> 0.1087\n",
      "http://192.168.1.50/login.php?user=guest -> 0.0000\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 -> 0.0000\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru -> 0.0000\n",
      "https://www.uni-mainz.de/ -> 0.9448\n",
      "Predict_proba smoke test: OK\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "cal = joblib.load(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "for u in tests:\n",
    "    Xinf = featureize_url(u)\n",
    "    p = float(cal.predict_proba(Xinf)[:,1][0])\n",
    "    assert 0.0 <= p <= 1.0, f\"Invalid probability {p} for {u}\"\n",
    "    print(f\"{u} -> {p:.4f}\")\n",
    "print(\"Predict_proba smoke test: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6532a173-fd2c-49ea-ab20-b0e355bdcb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsDomainIP: 1 TLD: other\n",
      "HasObfuscation: 1 NoOfObfuscatedChar: 5\n",
      "Edge-case unit tests: OK\n"
     ]
    }
   ],
   "source": [
    "# 1) IP host should set IsDomainIP=1 and TLD to 'other' or a valid category\n",
    "Xi = featureize_url(\"http://192.168.0.1/admin\")\n",
    "print(\"IsDomainIP:\", int(Xi.at[0,\"IsDomainIP\"]), \"TLD:\", str(Xi.at[0,\"TLD\"]))\n",
    "assert int(Xi.at[0,\"IsDomainIP\"]) == 1, \"IP detection failed\"\n",
    "assert str(Xi.at[0,\"TLD\"]) in TLD_CATS, \"TLD not in category space\"\n",
    "\n",
    "# 2) Obfuscation tokens should increment counts/ratio\n",
    "Xo = featureize_url(\"http://x.com/%2F%2E%2E/%2E%2E?u=1\")\n",
    "print(\"HasObfuscation:\", int(Xo.at[0,\"HasObfuscation\"]), \"NoOfObfuscatedChar:\", int(Xo.at[0,\"NoOfObfuscatedChar\"]))\n",
    "assert int(Xo.at[0,\"HasObfuscation\"]) == 1, \"Obfuscation flag failed\"\n",
    "assert int(Xo.at[0,\"NoOfObfuscatedChar\"]) >= 1, \"Obfuscated char count failed\"\n",
    "\n",
    "# 3) Ratios are within [0,1] and consistent with counts/length\n",
    "for Xs in [Xi, Xo]:\n",
    "    L = int(Xs.at[0,\"URLLength\"])\n",
    "    letters = int(Xs.at[0,\"NoOfLettersInURL\"])\n",
    "    digits  = int(Xs.at[0,\"NoOfDegitsInURL\"])\n",
    "    specials= int(Xs.at[0,\"NoOfOtherSpecialCharsInURL\"])\n",
    "    assert 0 <= Xs.at[0,\"LetterRatioInURL\"] <= 1 and 0 <= Xs.at[0,\"DegitRatioInURL\"] <= 1 and 0 <= Xs.at[0,\"SpacialCharRatioInURL\"] <= 1, \"Ratio out of bounds\"\n",
    "    assert letters + digits + specials >= L - 5, \"Counts vs length look inconsistent (allow small slack)\"\n",
    "print(\"Edge-case unit tests: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a55886a9-508b-4f14-998a-dcfc970a9650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featureize throughput: 188.8 URLs/sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "urls = [\"https://example.com/?q=test\"]*1000\n",
    "t0 = time.time()\n",
    "_ = [featureize_url(u) for u in urls]\n",
    "dt = time.time() - t0\n",
    "print(f\"Featureize throughput: {len(urls)/dt:.1f} URLs/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c858f943-2b6b-4ccb-a477-6870a3f8e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging Features for Misclassified URLs ---\n",
      "\n",
      "--- Features for: https://secure-paypa1.com.verify-account.co/reset?session=abc123 ---\n",
      "                                   0\n",
      "URLLength                         64\n",
      "DomainLength                      35\n",
      "IsDomainIP                         0\n",
      "TLD                               co\n",
      "TLDLength                          2\n",
      "NoOfSubDomain                      2\n",
      "HasObfuscation                     0\n",
      "NoOfObfuscatedChar                 0\n",
      "ObfuscationRatio                 0.0\n",
      "NoOfLettersInURL                  49\n",
      "LetterRatioInURL            0.765625\n",
      "NoOfDegitsInURL                    4\n",
      "DegitRatioInURL               0.0625\n",
      "NoOfEqualsInURL                    1\n",
      "NoOfQMarkInURL                     1\n",
      "NoOfAmpersandInURL                 0\n",
      "NoOfOtherSpecialCharsInURL        11\n",
      "SpacialCharRatioInURL       0.171875\n",
      "IsHTTPS                            1\n",
      "\n",
      "--- Features for: http://example.com/%2F%2E%2E/redirect?to=http://bad.ru ---\n",
      "                                   0\n",
      "URLLength                         54\n",
      "DomainLength                      11\n",
      "IsDomainIP                         0\n",
      "TLD                              com\n",
      "TLDLength                          3\n",
      "NoOfSubDomain                      0\n",
      "HasObfuscation                     1\n",
      "NoOfObfuscatedChar                 3\n",
      "ObfuscationRatio            0.055556\n",
      "NoOfLettersInURL                  36\n",
      "LetterRatioInURL            0.666667\n",
      "NoOfDegitsInURL                    3\n",
      "DegitRatioInURL             0.055556\n",
      "NoOfEqualsInURL                    1\n",
      "NoOfQMarkInURL                     1\n",
      "NoOfAmpersandInURL                 0\n",
      "NoOfOtherSpecialCharsInURL        15\n",
      "SpacialCharRatioInURL       0.277778\n",
      "IsHTTPS                            0\n",
      "\n",
      "--- Features for: https://www.uni-mainz.de/ ---\n",
      "                               0\n",
      "URLLength                     25\n",
      "DomainLength                  16\n",
      "IsDomainIP                     0\n",
      "TLD                           de\n",
      "TLDLength                      2\n",
      "NoOfSubDomain                  1\n",
      "HasObfuscation                 0\n",
      "NoOfObfuscatedChar             0\n",
      "ObfuscationRatio             0.0\n",
      "NoOfLettersInURL              18\n",
      "LetterRatioInURL            0.72\n",
      "NoOfDegitsInURL                0\n",
      "DegitRatioInURL              0.0\n",
      "NoOfEqualsInURL                0\n",
      "NoOfQMarkInURL                 0\n",
      "NoOfAmpersandInURL             0\n",
      "NoOfOtherSpecialCharsInURL     7\n",
      "SpacialCharRatioInURL       0.28\n",
      "IsHTTPS                        1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Ensure the latest featureize_url function is defined above this cell\n",
    "\n",
    "print(\"\\n--- Debugging Features for Misclassified URLs ---\")\n",
    "\n",
    "misclassified_urls = [\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Should be HIGH\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",      # Should be HIGH\n",
    "    \"https://www.uni-mainz.de/\",                                     # Should be LOW\n",
    "]\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "for u in misclassified_urls:\n",
    "    print(f\"\\n--- Features for: {u} ---\")\n",
    "    # Make sure featureize_url is using the latest patched version\n",
    "    Xinf = featureize_url(u)\n",
    "    if not Xinf.empty:\n",
    "        # Transpose (.T) makes it easier to read\n",
    "        print(Xinf.T)\n",
    "    else:\n",
    "        print(\"Featureization failed.\")\n",
    "\n",
    "# Reset display options if needed\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13e32387-5508-4ab0-ac82-3e7e259e523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace the old _count_other_specials with this ---\n",
    "def _count_other_specials(u: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts special characters EXCLUDING common URL structural/query chars.\n",
    "    Matches the likely definition used for the CSV features.\n",
    "    \"\"\"\n",
    "    # Exclude: letters, numbers, and common chars: / : . - _ ? = & %\n",
    "    other_chars = re.findall(r\"[^A-Za-z0-9/:.\\-_\\?=&%]\", u)\n",
    "    return len(other_chars)\n",
    "\n",
    "# --- AND ---\n",
    "\n",
    "# --- Find the main featureize_url function again ---\n",
    "# --- Inside it, find the line calculating 'others' and REPLACE it ---\n",
    "# Replace this line:\n",
    "# others  = len(re.findall(r\"[^A-Za-z0-9]\", u))\n",
    "# With this line:\n",
    "    others = _count_other_specials(u) # Use the refined helper function\n",
    "\n",
    "# (Make sure the rest of the featureize_url function remains the same as the last version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6ba4a0a-5d50-407b-9008-1a379583da29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Tests with Final Featureizer ---\n",
      "https://www.google.com/ -> 0.1087 (benign)\n",
      "http://192.168.1.50/login.php?user=guest -> 0.0 (benign)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 -> 0.0 (benign)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru -> 0.0 (benign)\n",
      "https://www.uni-mainz.de/ -> 0.9448 (PHISHING)\n"
     ]
    }
   ],
   "source": [
    "# --- Re-run this test cell AFTER applying the special char fix ---\n",
    "if cal:\n",
    "    print(\"\\n--- Running Tests with Final Featureizer ---\")\n",
    "    tests = [\n",
    "        \"https://www.google.com/\",\n",
    "        \"http://192.168.1.50/login.php?user=guest\",\n",
    "        \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Phishing\n",
    "        \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\", # Obfuscation/Phishing\n",
    "        \"https://www.uni-mainz.de/\", # Benign university\n",
    "    ]\n",
    "    for u in tests:\n",
    "        Xinf = featureize_url(u)\n",
    "        if not Xinf.empty and cal:\n",
    "            try:\n",
    "                p = float(cal.predict_proba(Xinf)[:,1][0])\n",
    "                label = \"PHISHING\" if p > 0.5 else \"benign\"\n",
    "                print(f\"{u} -> {round(p, 4)} ({label})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for {u}: {e}\")\n",
    "                print(\"Feature vector was:\")\n",
    "                print(Xinf.to_string())\n",
    "        else:\n",
    "            print(f\"Featureization failed or model not loaded for {u}, skipping prediction.\")\n",
    "else:\n",
    "    print(\"\\nTests skipped because model could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09c5b834-f225-4d9f-982a-36958ff82138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema loaded for featureizer setup.\n",
      "Final featureize_url function defined with corrected IP regex.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from pathlib import Path\n",
    "import numpy as np # Make sure numpy is imported\n",
    "\n",
    "# --- Load schema/model paths (ensure these paths are correct) ---\n",
    "SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "MODEL_PATH = Path(\"models/phish_url_hgb_cal.joblib\")\n",
    "\n",
    "# --- Load Schema Info ---\n",
    "try:\n",
    "    SCHEMA = json.load(open(SCHEMA_PATH, \"r\", encoding=\"utf-8\"))\n",
    "    FEATURE_COLS_FROM_SCHEMA = SCHEMA[\"feature_cols\"] # Keep original list for reference\n",
    "    TLD_CATS = SCHEMA[\"tld_categories\"]  # ordered list from JSON\n",
    "    print(\"Schema loaded for featureizer setup.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not load '{SCHEMA_PATH}'. Cannot define featureizer correctly.\")\n",
    "    FEATURE_COLS_FROM_SCHEMA = []\n",
    "    TLD_CATS = []\n",
    "\n",
    "# --- Helper Function for IP Check (Corrected Regex) ---\n",
    "def _is_ipv4(host: str) -> bool:\n",
    "    if not host: return False\n",
    "    # CORRECTED REGEX: Use {1,3} for repetition\n",
    "    m = re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", host)\n",
    "    if not m: return False\n",
    "    # Check if parts are valid byte values\n",
    "    try:\n",
    "        return all(0 <= int(p) <= 255 for p in host.split(\".\"))\n",
    "    except ValueError:\n",
    "        return False # Should not happen if regex matched, but safe\n",
    "\n",
    "# --- Helper Function for Refined Special Char Count ---\n",
    "def _count_other_specials(u: str) -> int:\n",
    "    # Exclude: letters, numbers, and common chars: / : . - _ ? = & %\n",
    "    other_chars = re.findall(r\"[^A-Za-z0-9/:.\\-_\\?=&%]\", u)\n",
    "    return len(other_chars)\n",
    "\n",
    "# --- Final Patched Featureizer Function ---\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    # Use the TLD_CATS loaded at the start of the cell\n",
    "    global TLD_CATS, FEATURE_COLS_FROM_SCHEMA # Access the globally loaded schema info\n",
    "\n",
    "    # Define the FULL list of features *including new ones* for this function\n",
    "    CURRENT_FEATURE_COLS = [\n",
    "        \"URLLength\", \"DomainLength\", \"IsDomainIP\", \"TLD\", \"TLDLength\",\n",
    "        \"NoOfSubDomain\", \"HasObfuscation\", \"NoOfObfuscatedChar\", \"ObfuscationRatio\",\n",
    "        \"NoOfLettersInURL\", \"LetterRatioInURL\", \"NoOfDegitsInURL\", \"DegitRatioInURL\",\n",
    "        \"NoOfEqualsInURL\", \"NoOfQMarkInURL\", \"NoOfAmpersandInURL\",\n",
    "        \"NoOfOtherSpecialCharsInURL\", \"SpacialCharRatioInURL\", \"IsHTTPS\",\n",
    "        \"ContainsAt\", \"HasRedirectWord\", \"BrandMismatchHint\" # New features added\n",
    "    ]\n",
    "\n",
    "    if not TLD_CATS:\n",
    "         print(\"Schema (TLD_CATS) not loaded, cannot featureize.\")\n",
    "         return pd.DataFrame(columns=CURRENT_FEATURE_COLS)\n",
    "\n",
    "    u = (url or \"\").strip()\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", u):\n",
    "        u = \"http://\" + u\n",
    "\n",
    "    # Initialize with defaults BEFORE try block\n",
    "    row = {col: 0 for col in CURRENT_FEATURE_COLS if col != 'TLD'}\n",
    "    row['TLD'] = \"other\" # Default TLD\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(u)\n",
    "        host = parsed.hostname or \"\"\n",
    "        ext = tldextract.extract(u)\n",
    "\n",
    "        # Use corrected IP check\n",
    "        is_ip = 1 if _is_ipv4(host) else 0\n",
    "\n",
    "        tld_raw = \"\"\n",
    "        if is_ip:\n",
    "            tld_norm = \"other\"\n",
    "        else:\n",
    "            tld_raw = (ext.suffix or \"unknown\").lower()\n",
    "            if tld_raw in TLD_CATS:\n",
    "                tld_norm = tld_raw\n",
    "            elif \"other\" in TLD_CATS:\n",
    "                tld_norm = \"other\"\n",
    "            else:\n",
    "                # Fallback if 'other' is somehow missing from TLD_CATS list\n",
    "                tld_norm = TLD_CATS[0] if TLD_CATS else \"unknown\"\n",
    "\n",
    "        url_len = len(u)\n",
    "        domain_parts = [p for p in [ext.subdomain, ext.domain, ext.suffix] if p]\n",
    "        domain = \".\".join(domain_parts) if not is_ip else host\n",
    "        domain_len = len(domain)\n",
    "\n",
    "        is_https = 1 if (parsed.scheme or \"\").lower() == \"https\" else 0\n",
    "        subdomain = ext.subdomain or \"\"\n",
    "        no_sub = 0 if is_ip or not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "        letters = len(re.findall(r\"[A-Za-z]\", u))\n",
    "        digits  = len(re.findall(r\"[0-9]\", u))\n",
    "        # Use refined special char count\n",
    "        others = _count_other_specials(u)\n",
    "        letter_ratio  = (letters/url_len) if url_len else 0.0\n",
    "        digit_ratio   = (digits/url_len)  if url_len else 0.0\n",
    "        special_ratio = (others / url_len) if url_len else 0.0 # Use refined count for ratio\n",
    "\n",
    "        qmarks = u.count(\"?\")\n",
    "        amps   = u.count(\"&\")\n",
    "        equals = u.count(\"=\")\n",
    "\n",
    "        U = u.upper()\n",
    "        has_obf = 1 if (\"@\" in u or re.search(r\"%[0-9A-F]{2}\", U)) else 0\n",
    "        no_obf = len(re.findall(r\"%[0-9A-F]{2}\", U))\n",
    "        obf_ratio = (no_obf/url_len) if url_len else 0.0\n",
    "\n",
    "        # --- Create the initial 'row' dictionary ---\n",
    "        row = {\n",
    "            \"URLLength\": url_len, \"DomainLength\": domain_len, \"IsDomainIP\": is_ip,\n",
    "            \"TLD\": tld_norm, \"TLDLength\": len(tld_raw),\n",
    "            \"NoOfSubDomain\": no_sub,\n",
    "            \"HasObfuscation\": has_obf, \"NoOfObfuscatedChar\": no_obf, \"ObfuscationRatio\": obf_ratio,\n",
    "            \"NoOfLettersInURL\": letters, \"LetterRatioInURL\": letter_ratio,\n",
    "            \"NoOfDegitsInURL\": digits, \"DegitRatioInURL\": digit_ratio,\n",
    "            \"NoOfEqualsInURL\": equals, \"NoOfQMarkInURL\": qmarks, \"NoOfAmpersandInURL\": amps,\n",
    "            \"NoOfOtherSpecialCharsInURL\": others, # Use refined count\n",
    "            \"SpacialCharRatioInURL\": special_ratio, # Use refined count for ratio\n",
    "            \"IsHTTPS\": is_https\n",
    "        }\n",
    "\n",
    "        # --- Calculate and add lexical flags ---\n",
    "        L = u.lower()\n",
    "        contains_at = 1 if \"@\" in u else 0\n",
    "        has_redirect_word = 1 if any(w in L for w in [\"redirect\", \"verify\", \"login\", \"account\", \"update\", \"signin\", \"auth\", \"confirm\"]) else 0\n",
    "        brand_mismatch_hint = 1 if any(p in L for p in [\"paypa1\", \"secure-paypa\", \"verify-account\", \"confirm-account\", \"support-\", \"-login\", \"account-update\"]) else 0\n",
    "\n",
    "        row.update({\n",
    "            \"ContainsAt\": contains_at,\n",
    "            \"HasRedirectWord\": has_redirect_word,\n",
    "            \"BrandMismatchHint\": brand_mismatch_hint\n",
    "        })\n",
    "        # --- End lexical flags ---\n",
    "\n",
    "        X = pd.DataFrame([row])\n",
    "\n",
    "        # Enforce schema: order, missing columns, dtypes\n",
    "        for c in CURRENT_FEATURE_COLS:\n",
    "            if c not in X.columns:\n",
    "                X[c] = 0\n",
    "\n",
    "        X = X[CURRENT_FEATURE_COLS] # Enforce column order\n",
    "\n",
    "        # Cast numerics safely first\n",
    "        for c in CURRENT_FEATURE_COLS:\n",
    "            if c != \"TLD\":\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0).astype(np.float64)\n",
    "\n",
    "        # Set categorical LAST, ensuring value is valid\n",
    "        if 'TLD' in X.columns:\n",
    "             # Ensure category exists, default to 'other' or first if needed\n",
    "             current_tld = X['TLD'].iloc[0]\n",
    "             if current_tld not in TLD_CATS:\n",
    "                  fallback_tld = \"other\" if \"other\" in TLD_CATS else (TLD_CATS[0] if TLD_CATS else \"unknown\")\n",
    "                  X['TLD'] = fallback_tld\n",
    "             X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "        else:\n",
    "             X['TLD'] = pd.Categorical([\"other\"], categories=TLD_CATS) # Should not happen\n",
    "\n",
    "        # Final check for NaNs (redundant but safe)\n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during featureization for URL {url}: {e}\")\n",
    "        # Create default row on error matching CURRENT_FEATURE_COLS\n",
    "        error_df = pd.DataFrame(columns=CURRENT_FEATURE_COLS)\n",
    "        error_df.loc[0] = 0\n",
    "        error_df[\"TLD\"] = pd.Categorical([\"other\"], categories=TLD_CATS)\n",
    "        numeric_cols = error_df.select_dtypes(include=np.number).columns\n",
    "        error_df[numeric_cols] = error_df[numeric_cols].fillna(0)\n",
    "        return error_df\n",
    "\n",
    "print(\"Final featureize_url function defined with corrected IP regex.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fb8f530-f5e9-4a14-8f28-318543667e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TLD categories from previous schema.\n",
      "Using a sample of 20000 URLs for quick retraining.\n",
      "Rebuilding training data with 22 features using final featureizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c8b7ea25f4431ca5339fbcfc23e0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featureizing URLs (Sample):   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data rebuilt. Splitting data...\n",
      "Starting sample model training...\n",
      "Starting sample model calibration...\n",
      "Calculating sample metrics...\n",
      "\n",
      "--- Sample Model Performance (22 Features) ---\n",
      "{'AUC': np.float64(0.9985258154399239), 'AP': np.float64(0.9973532797330447), 'F1@0.5': 0.9963179553822828, 'Prec@90%Rec': 0.576, 'Thr@90%Rec': np.float64(2.0217563472981963e-06)}\n",
      "\n",
      "Sample retraining complete! Model is in 'cal' variable.\n",
      "High Recall (90%) Threshold calculated: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python313\\Lib\\site-packages\\sklearn\\calibration.py:337: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "import os\n",
    "\n",
    "# --- 1. Define Updated Feature Columns ---\n",
    "# This MUST match the features produced by the featureize_url above\n",
    "UPDATED_FEATURE_COLS = [\n",
    "    \"URLLength\", \"DomainLength\", \"IsDomainIP\", \"TLD\", \"TLDLength\",\n",
    "    \"NoOfSubDomain\", \"HasObfuscation\", \"NoOfObfuscatedChar\", \"ObfuscationRatio\",\n",
    "    \"NoOfLettersInURL\", \"LetterRatioInURL\", \"NoOfDegitsInURL\", \"DegitRatioInURL\",\n",
    "    \"NoOfEqualsInURL\", \"NoOfQMarkInURL\", \"NoOfAmpersandInURL\",\n",
    "    \"NoOfOtherSpecialCharsInURL\", \"SpacialCharRatioInURL\", \"IsHTTPS\",\n",
    "    \"ContainsAt\", \"HasRedirectWord\", \"BrandMismatchHint\" # New features added\n",
    "]\n",
    "\n",
    "# --- 2. Load Schema (for TLD Cats) and Original Data ---\n",
    "try:\n",
    "    SCHEMA_PATH = Path(\"models/schema.json\") # Load existing schema for TLD cats\n",
    "    with open(SCHEMA_PATH, \"r\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    # Use TLD_CATS from the global scope (loaded in the previous cell)\n",
    "    # TLD_CATS = SCHEMA[\"tld_categories\"] # Or reload if needed\n",
    "    if not TLD_CATS: raise FileNotFoundError(\"TLD_CATS not available\")\n",
    "    print(\"Using TLD categories from previous schema.\")\n",
    "\n",
    "    # Reload original data\n",
    "    DATA_PATH = Path(\"PhiUSIIL_Phishing_URL_Dataset.csv\") # Adjust if needed\n",
    "    original_use_cols = [ # Load only needed original columns\n",
    "        \"URL\", \"label\", \"TLD\" # Need TLD to ensure categories align if reloading df\n",
    "    ]\n",
    "    df_orig = pd.read_csv(DATA_PATH, usecols=[\"URL\", \"label\"]).dropna() # Load only URL and label\n",
    "    n_samples = 20000\n",
    "    df_sample = df_orig.sample(n=min(n_samples, len(df_orig)), random_state=42)\n",
    "    print(f\"Using a sample of {len(df_sample)} URLs for quick retraining.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not load '{SCHEMA_PATH}' or '{DATA_PATH}'. Cannot proceed.\")\n",
    "    df_sample = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- 3. Rebuild X using the *final* featureizer ---\n",
    "if not df_sample.empty and TLD_CATS:\n",
    "    print(\"Rebuilding training data with 22 features using final featureizer...\")\n",
    "    urls = df_sample[\"URL\"].tolist()\n",
    "    rows = []\n",
    "    # Make sure the LATEST featureize_url (with fixes) is defined above!\n",
    "    for u in tqdm(urls, desc=\"Featureizing URLs (Sample)\"):\n",
    "        feature_dict = featureize_url(u).iloc[0].to_dict()\n",
    "        rows.append(feature_dict)\n",
    "\n",
    "    X_new = pd.DataFrame(rows)\n",
    "    # Ensure all columns exist and enforce order using UPDATED_FEATURE_COLS\n",
    "    for col in UPDATED_FEATURE_COLS:\n",
    "        if col not in X_new.columns: X_new[col] = 0\n",
    "    X_new = X_new[UPDATED_FEATURE_COLS]\n",
    "    y_new = df_sample[\"label\"].astype(int)\n",
    "\n",
    "    # Ensure dtypes (Numeric + TLD Categorical)\n",
    "    for c in UPDATED_FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X_new[c] = pd.to_numeric(X_new[c], errors='coerce').fillna(0)\n",
    "    # Use TLD_CATS loaded from the previous schema\n",
    "    X_new[\"TLD\"] = pd.Categorical(X_new[\"TLD\"], categories=TLD_CATS)\n",
    "\n",
    "    print(\"Sample training data rebuilt. Splitting data...\")\n",
    "    Xtr, Xva, ytr, yva = train_test_split(X_new, y_new, test_size=0.20, stratify=y_new, random_state=42)\n",
    "\n",
    "    # --- 4. Retrain Model (HGBT + Calibration) ---\n",
    "    print(\"Starting sample model training...\")\n",
    "    try:\n",
    "        cat_feature_index = UPDATED_FEATURE_COLS.index(\"TLD\")\n",
    "        categorical_features = [cat_feature_index]\n",
    "    except ValueError:\n",
    "        print(\"Warning: TLD column not found. Training without categorical feature.\")\n",
    "        categorical_features = None\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        max_leaf_nodes=31, learning_rate=0.08, max_iter=300,\n",
    "        validation_fraction=0.1, early_stopping=True, random_state=42,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "    hgb.fit(Xtr, ytr)\n",
    "\n",
    "    print(\"Starting sample model calibration...\")\n",
    "    # Overwrite 'cal' with the new model\n",
    "    cal = CalibratedClassifierCV(hgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "    cal.fit(Xva, yva)\n",
    "\n",
    "    print(\"Calculating sample metrics...\")\n",
    "    proba_sample = cal.predict_proba(Xva)[:,1]\n",
    "    pred_sample = (proba_sample >= 0.5).astype(int)\n",
    "    auc_sample = roc_auc_score(yva, proba_sample)\n",
    "    ap_sample = average_precision_score(yva, proba_sample)\n",
    "    f1_sample = f1_score(yva, pred_sample)\n",
    "    prec, rec, th = precision_recall_curve(yva, proba_sample)\n",
    "    idx = np.where(rec >= 0.90)[0]\n",
    "    p_at_90 = float(prec[idx[0]]) if len(idx) else float(\"nan\")\n",
    "    # Safer indexing for threshold\n",
    "    thr_at_90 = float(th[idx[0]-1]) if len(idx) > 0 and idx[0] > 0 else (th[0] if len(th) > 0 else np.nan)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Sample Model Performance (22 Features) ---\")\n",
    "    print({\"AUC\":auc_sample, \"AP\":ap_sample, \"F1@0.5\":f1_sample, \"Prec@90%Rec\":p_at_90, \"Thr@90%Rec\":thr_at_90})\n",
    "\n",
    "    # --- We still won't save this yet, just use 'cal' for testing ---\n",
    "    print(\"\\nSample retraining complete! Model is in 'cal' variable.\")\n",
    "    # --- Save the HIGH RECALL THRESHOLD for later use in the UI ---\n",
    "    high_recall_threshold = thr_at_90\n",
    "    print(f\"High Recall (90%) Threshold calculated: {high_recall_threshold:.4f}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping retraining due to data loading error or missing TLD_CATS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdb4d3ef-0fd5-423b-ab07-25d73680df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Tests with Sample Model (22 Features + Lexical) ---\n",
      "https://www.google.com/ -> 0.9683 (Std: PHISHING, HR: PHISHING)\n",
      "http://192.168.1.50/login.php?user=guest -> 0.0 (Std: benign, HR: PHISHING)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 -> 0.0 (Std: benign, HR: PHISHING)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru -> 0.0 (Std: benign, HR: PHISHING)\n",
      "https://www.uni-mainz.de/ -> 0.9853 (Std: PHISHING, HR: PHISHING)\n"
     ]
    }
   ],
   "source": [
    "# --- Run this test cell AFTER the sample retraining with lexical features ---\n",
    "if 'cal' in locals() and cal is not None and 'high_recall_threshold' in locals():\n",
    "    print(\"\\n--- Running Tests with Sample Model (22 Features + Lexical) ---\")\n",
    "    tests = [\n",
    "        \"https://www.google.com/\",\n",
    "        \"http://192.168.1.50/login.php?user=guest\", # IP Address\n",
    "        \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Phishing\n",
    "        \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\", # Obfuscation/Phishing\n",
    "        \"https://www.uni-mainz.de/\", # Benign university\n",
    "    ]\n",
    "    # Make sure the LATEST featureize_url is defined above\n",
    "    for u in tests:\n",
    "        # Use the featureizer defined in Cell 1\n",
    "        Xinf = featureize_url(u)\n",
    "        if not Xinf.empty and cal:\n",
    "            try:\n",
    "                # Use the 'cal' model retrained in Cell 2\n",
    "                p = float(cal.predict_proba(Xinf)[:,1][0])\n",
    "                label_std = \"PHISHING\" if p > 0.5 else \"benign\"\n",
    "                # Use the calculated high recall threshold\n",
    "                label_hr = \"PHISHING\" if p > high_recall_threshold else \"benign\"\n",
    "                print(f\"{u} -> {round(p, 4)} (Std: {label_std}, HR: {label_hr})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for {u}: {e}\")\n",
    "        else:\n",
    "            print(f\"Featureization failed or model not loaded for {u}, skipping prediction.\")\n",
    "else:\n",
    "    print(\"\\nTests skipped because sample model ('cal') or 'high_recall_threshold' is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f02f78d5-1160-44fc-8ab3-eefb772a418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TLD categories from previous schema.\n",
      "Using FULL dataset of 235795 URLs for final retraining.\n",
      "Rebuilding training data with 22 features (Full Dataset)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6192544d6649a7b318e08d65188894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Featureizing URLs (Full):   0%|          | 0/235795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data rebuilt. Splitting data...\n",
      "Starting final model training...\n",
      "Starting final model calibration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python313\\Lib\\site-packages\\sklearn\\calibration.py:337: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final metrics...\n",
      "\n",
      "--- Final Model Performance (22 Features, Full Data) ---\n",
      "{'AUC': np.float64(0.9988087682633816), 'AP': np.float64(0.9986167563924571), 'F1@0.5': 0.9971135699219184, 'Prec@90%Rec': 0.5718950783519583, 'Thr@90%Rec': np.float64(2.8381343290608353e-11)}\n",
      "\n",
      "Saving final model and schema...\n",
      "Final High Recall (90%) Threshold calculated: 0.0000\n",
      "\n",
      "Full retraining complete! Final model saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell to Retrain on FULL Dataset ---\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # Use notebook version\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "import os\n",
    "\n",
    "# --- 1. Define Updated Feature Columns (MUST match featureize_url) ---\n",
    "UPDATED_FEATURE_COLS = [\n",
    "    \"URLLength\", \"DomainLength\", \"IsDomainIP\", \"TLD\", \"TLDLength\",\n",
    "    \"NoOfSubDomain\", \"HasObfuscation\", \"NoOfObfuscatedChar\", \"ObfuscationRatio\",\n",
    "    \"NoOfLettersInURL\", \"LetterRatioInURL\", \"NoOfDegitsInURL\", \"DegitRatioInURL\",\n",
    "    \"NoOfEqualsInURL\", \"NoOfQMarkInURL\", \"NoOfAmpersandInURL\",\n",
    "    \"NoOfOtherSpecialCharsInURL\", \"SpacialCharRatioInURL\", \"IsHTTPS\",\n",
    "    \"ContainsAt\", \"HasRedirectWord\", \"BrandMismatchHint\"\n",
    "]\n",
    "\n",
    "# --- 2. Load Schema (for TLD Cats) and FULL Original Data ---\n",
    "try:\n",
    "    SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "    with open(SCHEMA_PATH, \"r\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    # Use TLD_CATS from the global scope (loaded when defining featureize_url)\n",
    "    if not TLD_CATS: raise FileNotFoundError(\"TLD_CATS not available\")\n",
    "    print(\"Using TLD categories from previous schema.\")\n",
    "\n",
    "    # Load FULL original data\n",
    "    DATA_PATH = Path(\"PhiUSIIL_Phishing_URL_Dataset.csv\") # Adjust if needed\n",
    "    df_orig = pd.read_csv(DATA_PATH, usecols=[\"URL\", \"label\"]).dropna()\n",
    "    print(f\"Using FULL dataset of {len(df_orig)} URLs for final retraining.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not load '{SCHEMA_PATH}' or '{DATA_PATH}'. Cannot proceed.\")\n",
    "    df_orig = pd.DataFrame() # Make df empty\n",
    "\n",
    "\n",
    "# --- 3. Rebuild X using the final featureizer on FULL data ---\n",
    "if not df_orig.empty and TLD_CATS:\n",
    "    print(\"Rebuilding training data with 22 features (Full Dataset)...\")\n",
    "    urls = df_orig[\"URL\"].tolist()\n",
    "    rows = []\n",
    "    # Make sure the LATEST featureize_url (with fixes) is defined above!\n",
    "    for u in tqdm(urls, desc=\"Featureizing URLs (Full)\"):\n",
    "        feature_dict = featureize_url(u).iloc[0].to_dict()\n",
    "        rows.append(feature_dict)\n",
    "\n",
    "    X_full = pd.DataFrame(rows)\n",
    "    # Ensure all columns exist and enforce order\n",
    "    for col in UPDATED_FEATURE_COLS:\n",
    "        if col not in X_full.columns: X_full[col] = 0\n",
    "    X_full = X_full[UPDATED_FEATURE_COLS]\n",
    "    y_full = df_orig[\"label\"].astype(int)\n",
    "\n",
    "    # Ensure dtypes\n",
    "    for c in UPDATED_FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X_full[c] = pd.to_numeric(X_full[c], errors='coerce').fillna(0)\n",
    "    X_full[\"TLD\"] = pd.Categorical(X_full[\"TLD\"], categories=TLD_CATS)\n",
    "\n",
    "    print(\"Full training data rebuilt. Splitting data...\")\n",
    "    Xtr, Xva, ytr, yva = train_test_split(X_full, y_full, test_size=0.20, stratify=y_full, random_state=42)\n",
    "\n",
    "    # --- 4. Retrain Final Model (HGBT + Calibration) ---\n",
    "    print(\"Starting final model training...\")\n",
    "    try:\n",
    "        cat_feature_index = UPDATED_FEATURE_COLS.index(\"TLD\")\n",
    "        categorical_features = [cat_feature_index]\n",
    "    except ValueError:\n",
    "        print(\"Warning: TLD column not found. Training without categorical feature.\")\n",
    "        categorical_features = None\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        max_leaf_nodes=31, learning_rate=0.08, max_iter=300, # Consider slightly increasing max_iter if needed\n",
    "        validation_fraction=0.1, early_stopping=True, random_state=42,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "    hgb.fit(Xtr, ytr)\n",
    "\n",
    "    print(\"Starting final model calibration...\")\n",
    "    # Overwrite 'cal' with the FINAL model\n",
    "    cal = CalibratedClassifierCV(hgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "    cal.fit(Xva, yva)\n",
    "\n",
    "    print(\"Calculating final metrics...\")\n",
    "    proba_final = cal.predict_proba(Xva)[:,1]\n",
    "    pred_final = (proba_final >= 0.5).astype(int)\n",
    "    auc_final = roc_auc_score(yva, proba_final)\n",
    "    ap_final = average_precision_score(yva, proba_final)\n",
    "    f1_final = f1_score(yva, pred_final)\n",
    "    prec, rec, th = precision_recall_curve(yva, proba_final)\n",
    "    idx = np.where(rec >= 0.90)[0]\n",
    "    p_at_90 = float(prec[idx[0]]) if len(idx) else float(\"nan\")\n",
    "    thr_at_90 = float(th[idx[0]-1]) if len(idx) > 0 and idx[0] > 0 else (th[0] if len(th) > 0 else np.nan)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Model Performance (22 Features, Full Data) ---\")\n",
    "    print({\"AUC\":auc_final, \"AP\":ap_final, \"F1@0.5\":f1_final, \"Prec@90%Rec\":p_at_90, \"Thr@90%Rec\":thr_at_90})\n",
    "\n",
    "    # --- SAVE the FINAL Model and Schema ---\n",
    "    print(\"\\nSaving final model and schema...\")\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    joblib.dump(cal, \"models/phish_url_hgb_cal.joblib\") # Save the final 'cal' model\n",
    "    # Use the UPDATED_FEATURE_COLS for the final schema\n",
    "    FINAL_SCHEMA = {\"feature_cols\": UPDATED_FEATURE_COLS, \"tld_categories\": TLD_CATS}\n",
    "    json.dump(FINAL_SCHEMA, open(\"models/schema.json\",\"w\"), ensure_ascii=False) # Overwrite schema\n",
    "\n",
    "    # --- Store the final High Recall Threshold ---\n",
    "    high_recall_threshold = thr_at_90\n",
    "    print(f\"Final High Recall (90%) Threshold calculated: {high_recall_threshold:.4f}\")\n",
    "    print(\"\\nFull retraining complete! Final model saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping retraining due to data loading error or missing TLD_CATS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24790940-f5ed-408a-91ac-37ae72a48973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the featureize_url function defined in the notebook session.\n",
      "Final model and schema loaded successfully.\n",
      "Using High Recall Threshold: 2.8381e-11\n",
      "\n",
      "--- Running Tests with FINAL Model ---\n",
      "https://www.google.com/ -> 0.1945 (Std: benign, HR: PHISHING)\n",
      "http://192.168.1.50/login.php?user=guest -> 0.0 (Std: benign, HR: PHISHING)\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123 -> 0.0 (Std: benign, HR: PHISHING)\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru -> 0.0 (Std: benign, HR: PHISHING)\n",
      "https://www.uni-mainz.de/ -> 0.9923 (Std: PHISHING, HR: PHISHING)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Make sure the FINAL featureize_url function definition is active ---\n",
    "# (It should be defined in a cell above this one)\n",
    "print(\"Using the featureize_url function defined in the notebook session.\")\n",
    "\n",
    "# --- 1. Load the FINAL Model and Schema ---\n",
    "try:\n",
    "    SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "    MODEL_PATH = Path(\"models/phish_url_hgb_cal.joblib\") # Final saved model\n",
    "\n",
    "    with open(SCHEMA_PATH, \"r\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    # Load the FINAL calibrated model\n",
    "    cal_final = joblib.load(MODEL_PATH)\n",
    "    # Load the high recall threshold from the schema if saved, else use the value from training output\n",
    "    # NOTE: Adjust this value if needed based on your final training output\n",
    "    high_recall_threshold = SCHEMA.get(\"high_recall_threshold\", 2.8381343290608353e-11)\n",
    "\n",
    "    print(\"Final model and schema loaded successfully.\")\n",
    "    print(f\"Using High Recall Threshold: {high_recall_threshold:.4e}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: Could not load '{SCHEMA_PATH}' or '{MODEL_PATH}'. Cannot test.\")\n",
    "    cal_final = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/schema: {e}\")\n",
    "    cal_final = None\n",
    "\n",
    "# --- 2. Run Tests ---\n",
    "if cal_final:\n",
    "    print(\"\\n--- Running Tests with FINAL Model ---\")\n",
    "    tests = [\n",
    "        \"https://www.google.com/\",\n",
    "        \"http://192.168.1.50/login.php?user=guest\", # IP Address\n",
    "        \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\", # Phishing\n",
    "        \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\", # Obfuscation/Phishing\n",
    "        \"https://www.uni-mainz.de/\", # Benign university\n",
    "    ]\n",
    "    for u in tests:\n",
    "        # Use the featureize_url defined in this notebook session\n",
    "        Xinf = featureize_url(u)\n",
    "        if not Xinf.empty and cal_final:\n",
    "            try:\n",
    "                # Use the final loaded model\n",
    "                p = float(cal_final.predict_proba(Xinf)[:,1][0])\n",
    "                label_std = \"PHISHING\" if p > 0.5 else \"benign\"\n",
    "                # Use the final high recall threshold\n",
    "                label_hr = \"PHISHING\" if p > high_recall_threshold else \"benign\"\n",
    "                print(f\"{u} -> {round(p, 4)} (Std: {label_std}, HR: {label_hr})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for {u}: {e}\")\n",
    "        else:\n",
    "            print(f\"Featureization failed or model not loaded for {u}, skipping prediction.\")\n",
    "else:\n",
    "    print(\"\\nTests skipped because final model ('cal_final') could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f563c4f4-2064-4bac-be83-6ba96b8b6703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featureizer: Schema loaded successfully.\n",
      "Running featureizer self-test...\n",
      "--- Features for: https://secure-paypa1.com.verify-account.co/reset?session=abc123 ---\n",
      "                                   0\n",
      "URLLength                       64.0\n",
      "DomainLength                    35.0\n",
      "IsDomainIP                       0.0\n",
      "TLD                               co\n",
      "TLDLength                        2.0\n",
      "NoOfSubDomain                    2.0\n",
      "HasObfuscation                   0.0\n",
      "NoOfObfuscatedChar               0.0\n",
      "ObfuscationRatio                 0.0\n",
      "NoOfLettersInURL                49.0\n",
      "LetterRatioInURL            0.765625\n",
      "NoOfDegitsInURL                  4.0\n",
      "DegitRatioInURL               0.0625\n",
      "NoOfEqualsInURL                  1.0\n",
      "NoOfQMarkInURL                   1.0\n",
      "NoOfAmpersandInURL               0.0\n",
      "NoOfOtherSpecialCharsInURL       0.0\n",
      "SpacialCharRatioInURL            0.0\n",
      "IsHTTPS                          1.0\n",
      "ContainsAt                       0.0\n",
      "HasRedirectWord                  1.0\n",
      "BrandMismatchHint                1.0\n",
      "\n",
      "Self-test complete. Check 'BrandMismatchHint' and 'HasRedirectWord' are 1.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load the schema file ---\n",
    "try:\n",
    "    # This path logic works when imported by main.py (which is in the root)\n",
    "    SCHEMA_PATH = Path(\"models/schema.json\")\n",
    "    with open(SCHEMA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        SCHEMA = json.load(f)\n",
    "    \n",
    "    FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "    TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "    print(\"Featureizer: Schema loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL (Featureizer): Could not load schema from {SCHEMA_PATH}.\")\n",
    "    FEATURE_COLS = []\n",
    "    TLD_CATS = []\n",
    "\n",
    "# --- Helper Function for IP Check (Corrected Regex) ---\n",
    "def _is_ipv4(host: str) -> bool:\n",
    "    if not host: return False\n",
    "    # CORRECTED REGEX: Use {1,3} for repetition\n",
    "    m = re.fullmatch(r\"\\d{1,3}(\\.\\d{1,3}){3}\", host)\n",
    "    if not m: return False\n",
    "    try:\n",
    "        return all(0 <= int(p) <= 255 for p in host.split(\".\"))\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# --- Helper Function for Refined Special Char Count ---\n",
    "def _count_other_specials(u: str) -> int:\n",
    "    # Exclude: letters, numbers, and common chars: / : . - _ ? = & %\n",
    "    other_chars = re.findall(r\"[^A-Za-z0-9/:.\\-_\\?=&%]\", u)\n",
    "    return len(other_chars)\n",
    "\n",
    "# --- Final Patched Featureizer Function ---\n",
    "def featureize_url(url: str) -> pd.DataFrame:\n",
    "    global TLD_CATS, FEATURE_COLS\n",
    "\n",
    "    # Use the full list of 22 features from the loaded schema\n",
    "    CURRENT_FEATURE_COLS = FEATURE_COLS \n",
    "\n",
    "    if not TLD_CATS or not CURRENT_FEATURE_COLS:\n",
    "         print(\"Error (Featureizer): Schema not loaded, cannot featureize.\")\n",
    "         return pd.DataFrame(columns=CURRENT_FEATURE_COLS)\n",
    "\n",
    "    u = (url or \"\").strip()\n",
    "    if not re.match(r\"^[a-zA-Z]+://\", u):\n",
    "        u = \"http://\" + u\n",
    "\n",
    "    # Initialize with defaults\n",
    "    row = {col: 0 for col in CURRENT_FEATURE_COLS if col != 'TLD'}\n",
    "    row['TLD'] = \"other\"\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(u)\n",
    "        host = parsed.hostname or \"\"\n",
    "        ext = tldextract.extract(u)\n",
    "        is_ip = 1 if _is_ipv4(host) else 0\n",
    "        tld_raw = \"\"\n",
    "\n",
    "        if is_ip:\n",
    "            tld_norm = \"other\"\n",
    "        else:\n",
    "            tld_raw = (ext.suffix or \"unknown\").lower()\n",
    "            if tld_raw in TLD_CATS:\n",
    "                tld_norm = tld_raw\n",
    "            elif \"other\" in TLD_CATS:\n",
    "                tld_norm = \"other\"\n",
    "            else:\n",
    "                tld_norm = TLD_CATS[0] # Fallback\n",
    "\n",
    "        url_len = len(u)\n",
    "        domain_parts = [p for p in [ext.subdomain, ext.domain, ext.suffix] if p]\n",
    "        domain = \".\".join(domain_parts) if not is_ip else host\n",
    "        domain_len = len(domain)\n",
    "        is_https = 1 if (parsed.scheme or \"\").lower() == \"https\" else 0\n",
    "        subdomain = ext.subdomain or \"\"\n",
    "        no_sub = 0 if is_ip or not subdomain else subdomain.count(\".\") + 1\n",
    "\n",
    "        letters = len(re.findall(r\"[A-Za-z]\", u))\n",
    "        digits  = len(re.findall(r\"[0-9]\", u))\n",
    "        others = _count_other_specials(u) # Use refined count\n",
    "        letter_ratio  = (letters/url_len) if url_len else 0.0\n",
    "        digit_ratio   = (digits/url_len)  if url_len else 0.0\n",
    "        special_ratio = (others / url_len) if url_len else 0.0\n",
    "\n",
    "        qmarks = u.count(\"?\")\n",
    "        amps   = u.count(\"&\")\n",
    "        equals = u.count(\"=\")\n",
    "\n",
    "        U = u.upper()\n",
    "        has_obf = 1 if (\"@\" in u or re.search(r\"%[0-9A-F]{2}\", U)) else 0\n",
    "        no_obf = len(re.findall(r\"%[0-9A-F]{2}\", U))\n",
    "        obf_ratio = (no_obf/url_len) if url_len else 0.0\n",
    "\n",
    "        # --- Create the initial 'row' dictionary ---\n",
    "        row = {\n",
    "            \"URLLength\": url_len, \"DomainLength\": domain_len, \"IsDomainIP\": is_ip,\n",
    "            \"TLD\": tld_norm, \"TLDLength\": len(tld_raw),\n",
    "            \"NoOfSubDomain\": no_sub,\n",
    "            \"HasObfuscation\": has_obf, \"NoOfObfuscatedChar\": no_obf, \"ObfuscationRatio\": obf_ratio,\n",
    "            \"NoOfLettersInURL\": letters, \"LetterRatioInURL\": letter_ratio,\n",
    "            \"NoOfDegitsInURL\": digits, \"DegitRatioInURL\": digit_ratio,\n",
    "            \"NoOfEqualsInURL\": equals, \"NoOfQMarkInURL\": qmarks, \"NoOfAmpersandInURL\": amps,\n",
    "            \"NoOfOtherSpecialCharsInURL\": others,\n",
    "            \"SpacialCharRatioInURL\": special_ratio,\n",
    "            \"IsHTTPS\": is_https\n",
    "        }\n",
    "\n",
    "        # --- Calculate and add lexical flags ---\n",
    "        L = u.lower()\n",
    "        contains_at = 1 if \"@\" in u else 0\n",
    "        has_redirect_word = 1 if any(w in L for w in [\"redirect\", \"verify\", \"login\", \"account\", \"update\", \"signin\", \"auth\", \"confirm\"]) else 0\n",
    "        brand_mismatch_hint = 1 if any(p in L for p in [\"paypa1\", \"secure-paypa\", \"verify-account\", \"confirm-account\", \"support-\", \"-login\", \"account-update\"]) else 0\n",
    "\n",
    "        row.update({\n",
    "            \"ContainsAt\": contains_at,\n",
    "            \"HasRedirectWord\": has_redirect_word,\n",
    "            \"BrandMismatchHint\": brand_mismatch_hint\n",
    "        })\n",
    "        # --- End lexical flags ---\n",
    "\n",
    "        X = pd.DataFrame([row])\n",
    "\n",
    "        # Enforce schema: order, missing columns, dtypes\n",
    "        for c in CURRENT_FEATURE_COLS:\n",
    "            if c not in X.columns:\n",
    "                X[c] = 0\n",
    "        X = X[CURRENT_FEATURE_COLS]\n",
    "\n",
    "        for c in CURRENT_FEATURE_COLS:\n",
    "            if c != \"TLD\":\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0).astype(np.float64)\n",
    "\n",
    "        if 'TLD' in X.columns:\n",
    "             current_tld = X['TLD'].iloc[0]\n",
    "             if current_tld not in TLD_CATS:\n",
    "                  fallback_tld = \"other\" if \"other\" in TLD_CATS else TLD_CATS[0]\n",
    "                  X['TLD'] = fallback_tld\n",
    "             X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "        \n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during featureization for URL {url}: {e}\")\n",
    "        error_df = pd.DataFrame(columns=CURRENT_FEATURE_COLS)\n",
    "        error_df.loc[0] = 0\n",
    "        error_df[\"TLD\"] = pd.Categorical([\"other\"], categories=TLD_CATS)\n",
    "        numeric_cols = error_df.select_dtypes(include=np.number).columns\n",
    "        error_df[numeric_cols] = error_df[numeric_cols].fillna(0)\n",
    "        return error_df\n",
    "\n",
    "# --- Self-Test (to run this file directly) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # This test will only work if models/schema.json is in the parent directory\n",
    "    # relative to where you run this script.\n",
    "    print(\"Running featureizer self-test...\")\n",
    "    test_url = \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\"\n",
    "    features_df = featureize_url(test_url)\n",
    "    print(f\"--- Features for: {test_url} ---\")\n",
    "    print(features_df.T.to_string())\n",
    "    print(\"\\nSelf-test complete. Check 'BrandMismatchHint' and 'HasRedirectWord' are 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fda069a7-7f36-401a-89eb-0c8d120fcd1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'featureizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m     sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# If it's inside a 'scripts' folder, add that instead:\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# scripts_dir = project_root / \"scripts\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# scripts_dir.mkdir(exist_ok=True)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# if str(scripts_dir) not in sys.path:\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     sys.path.insert(0, str(scripts_dir))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeatureizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFZ\u001b[39;00m\n\u001b[32m     15\u001b[39m importlib.reload(FZ)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfeatureizer imported. has features_from_url:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mhasattr\u001b[39m(FZ, \u001b[33m\"\u001b[39m\u001b[33mfeatures_from_url\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'featureizer'"
     ]
    }
   ],
   "source": [
    "# Make the current directory importable and import featureizer.py\n",
    "import sys, pathlib, importlib\n",
    "\n",
    "project_root = pathlib.Path(\".\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# If it's inside a 'scripts' folder, add that instead:\n",
    "# scripts_dir = project_root / \"scripts\"\n",
    "# scripts_dir.mkdir(exist_ok=True)\n",
    "# if str(scripts_dir) not in sys.path:\n",
    "#     sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "import featureizer as FZ\n",
    "importlib.reload(FZ)\n",
    "print(\"featureizer imported. has features_from_url:\", hasattr(FZ, \"features_from_url\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "994afab7-e670-4b2e-a7c7-7bbc582f3031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols: 22 ['URLLength', 'DomainLength', 'IsDomainIP', 'TLD', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS', 'ContainsAt', 'HasRedirectWord', 'BrandMismatchHint']\n",
      "TLD dtype: category\n",
      "Sample row:\n",
      " {'URLLength': 64.0, 'DomainLength': 35.0, 'IsDomainIP': 0.0, 'TLD': 'co', 'TLDLength': 2.0, 'NoOfSubDomain': 2.0, 'HasObfuscation': 0.0, 'NoOfObfuscatedChar': 0.0, 'ObfuscationRatio': 0.0, 'NoOfLettersInURL': 49.0, 'LetterRatioInURL': 0.765625, 'NoOfDegitsInURL': 4.0, 'DegitRatioInURL': 0.0625, 'NoOfEqualsInURL': 1.0, 'NoOfQMarkInURL': 1.0, 'NoOfAmpersandInURL': 0.0, 'NoOfOtherSpecialCharsInURL': 0.0, 'SpacialCharRatioInURL': 0.0, 'IsHTTPS': 1.0, 'ContainsAt': 0.0, 'HasRedirectWord': 1.0, 'BrandMismatchHint': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Self-check: confirm 22 features and correct order\n",
    "df = featureize_url(\"https://secure-paypa1.com.verify-account.co/reset?session=abc123\")\n",
    "print(\"Cols:\", len(df.columns), list(df.columns))\n",
    "print(\"TLD dtype:\", df[\"TLD\"].dtype)\n",
    "print(\"Sample row:\\n\", df.iloc[0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35df3f1e-3986-45e2-889e-b06c5a765750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model + schema\n",
      "Imported featureizer from: C:\\Python313\\Scripts\\ AI Phishing & Malicious Link Analyzer\\scripts\n",
      "Schema/dtype checks: OK\n",
      "HR threshold (demo): 0.20\n",
      "https://www.google.com/\n",
      "  prob=0.1170 | Std=benign | HR=benign\n",
      "http://192.168.1.50/login.php?user=guest\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "https://www.uni-mainz.de/\n",
      "  prob=0.9871 | Std=PHISHING | HR=PHISHING\n",
      "Total latency: 172.7 ms for 5 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sampa\\AppData\\Local\\Temp\\ipykernel_19068\\614515693.py:65: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  assert pd.api.types.is_categorical_dtype(X0[\"TLD\"]), \"TLD not categorical\"\n"
     ]
    }
   ],
   "source": [
    "# === Final Model Smoke Test (Jupyter) ===\n",
    "import sys, pathlib, importlib, json, joblib, pandas as pd, numpy as np, time\n",
    "\n",
    "# 1) Resolve paths: set repo root and common folders\n",
    "ROOT = pathlib.Path(\".\").resolve()  # if notebook is nested, adjust: pathlib.Path(\"..\").resolve()\n",
    "MODELS = ROOT / \"models\"\n",
    "SCRIPTS = ROOT / \"scripts\"\n",
    "APP = ROOT / \"app\"\n",
    "\n",
    "assert (MODELS / \"schema.json\").exists(), f\"Missing: {MODELS/'schema.json'}\"\n",
    "assert (MODELS / \"phish_url_hgb_cal.joblib\").exists(), f\"Missing: {MODELS/'phish_url_hgb_cal.joblib'}\"\n",
    "\n",
    "# 2) Load schema + model\n",
    "SCHEMA = json.load(open(MODELS / \"schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "cal = joblib.load(MODELS / \"phish_url_hgb_cal.joblib\")\n",
    "print(\"Loaded model + schema\")\n",
    "\n",
    "# 3) Import featureizer: try scripts/featureizer.py then app/featureizer.py\n",
    "FZ = None\n",
    "for candidate in [SCRIPTS, APP]:\n",
    "    if candidate.exists():\n",
    "        if str(candidate) not in sys.path:\n",
    "            sys.path.insert(0, str(candidate))\n",
    "        try:\n",
    "            import featureizer as _tmp\n",
    "            importlib.reload(_tmp)\n",
    "            FZ = _tmp\n",
    "            print(f\"Imported featureizer from: {candidate}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "assert FZ is not None, \"Could not import featureizer.py from scripts/ or app/\"\n",
    "\n",
    "assert hasattr(FZ, \"features_from_url\"), \"featureizer.features_from_url not found\"\n",
    "\n",
    "# 4) Wrapper to enforce schema order + dtypes (parity guard)\n",
    "def to_aligned_df(url: str) -> pd.DataFrame:\n",
    "    out = FZ.features_from_url(url)\n",
    "    X = pd.DataFrame([out]) if isinstance(out, dict) else out.copy()\n",
    "    # add missing and enforce order\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    # numerics then categorical\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = X[\"TLD\"].astype(str).str.lower().apply(\n",
    "        lambda t: t if t in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "    )\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    # final numeric safety\n",
    "    num_cols = [c for c in FEATURE_COLS if c != \"TLD\"]\n",
    "    X[num_cols] = X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return X\n",
    "\n",
    "# 5) Schema/dtype sanity\n",
    "X0 = to_aligned_df(\"https://example.com/?q=1\")\n",
    "assert list(X0.columns) == FEATURE_COLS, \"Column order mismatch\"\n",
    "for c in FEATURE_COLS:\n",
    "    if c != \"TLD\":\n",
    "        assert pd.api.types.is_numeric_dtype(X0[c]), f\"{c} not numeric\"\n",
    "assert pd.api.types.is_categorical_dtype(X0[\"TLD\"]), \"TLD not categorical\"\n",
    "print(\"Schema/dtype checks: OK\")\n",
    "\n",
    "# 6) Choose a demo-friendly High Recall threshold (saved one was ~0)\n",
    "HR_THRESHOLD = 0.20\n",
    "print(f\"HR threshold (demo): {HR_THRESHOLD:.2f}\")\n",
    "\n",
    "# 7) Score a small set\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "t0 = time.time()\n",
    "for u in tests:\n",
    "    X = to_aligned_df(u)\n",
    "    p = float(cal.predict_proba(X)[:,1][0])\n",
    "    std = \"PHISHING\" if p>=0.5 else \"benign\"\n",
    "    hr  = \"PHISHING\" if p>=HR_THRESHOLD else \"benign\"\n",
    "    print(f\"{u}\\n  prob={p:.4f} | Std={std} | HR={hr}\")\n",
    "print(f\"Total latency: {(time.time()-t0)*1000:.1f} ms for {len(tests)} URLs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03df31aa-9267-47e5-8433-35d9e24c2498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438d3037fec04764a4635f6b0ee1cafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<h3 style='margin:8px 0;color:#e5e7eb;'>PhishGuard — URL Risk (Notebook)</h3>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594b9e6d5aeb443aa0e2be8b9aef8bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='https://www.google.com/', description='Input', layout=Layout(width='100%'), placeho…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6efcdb15-a63d-4891-b87a-a275fde22081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready for UI integration!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Javascript\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Load your model and featureizer\n",
    "ROOT = Path(\".\").resolve()\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "# Load schema + model\n",
    "SCHEMA = json.load(open(MODELS / \"schema.json\", \"r\", encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "cal = joblib.load(MODELS / \"phish_url_hgb_cal.joblib\")\n",
    "\n",
    "# Import featureizer\n",
    "sys.path.insert(0, str(ROOT / \"scripts\"))\n",
    "from featureizer import features_from_url\n",
    "\n",
    "# Wrapper function (same as your working version)\n",
    "def to_aligned_df(url: str) -> pd.DataFrame:\n",
    "    out = features_from_url(url)\n",
    "    X = pd.DataFrame([out]) if isinstance(out, dict) else out.copy()\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = X[\"TLD\"].astype(str).str.lower().apply(\n",
    "        lambda t: t if t in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "    )\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    num_cols = [c for c in FEATURE_COLS if c != \"TLD\"]\n",
    "    X[num_cols] = X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return X\n",
    "\n",
    "# Prediction function\n",
    "def predict_url(url):\n",
    "    try:\n",
    "        X = to_aligned_df(url)\n",
    "        probability = float(cal.predict_proba(X)[:, 1][0])\n",
    "        return probability\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Model loaded and ready for UI integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcfbeb0c-a077-471a-badb-9ea777ee242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\python313\\lib\\site-packages (from Flask) (8.2.1)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\python313\\lib\\site-packages (from Flask) (3.1.5)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\python313\\lib\\site-packages (from Flask) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\python313\\lib\\site-packages (from Flask) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from click>=8.1.3->Flask) (0.4.6)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, blinker, Flask\n",
      "\n",
      "   -------------------------- ------------- 2/3 [Flask]\n",
      "   ---------------------------------------- 3/3 [Flask]\n",
      "\n",
      "Successfully installed Flask-3.1.2 blinker-1.9.0 itsdangerous-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7087859-dd99-4227-ba52-e533c3493b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready for UI integration!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup backend API in Jupyter\n",
    "from IPython.display import display, HTML, Javascript\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from flask import Flask, jsonify, request\n",
    "import threading\n",
    "\n",
    "# Load your model (same as your working code)\n",
    "ROOT = Path(\".\").resolve()\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "# Load schema + model\n",
    "SCHEMA = json.load(open(MODELS / \"schema.json\", \"r\", encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "cal = joblib.load(MODELS / \"phish_url_hgb_cal.joblib\")\n",
    "\n",
    "# Import featureizer\n",
    "sys.path.insert(0, str(ROOT / \"scripts\"))\n",
    "from featureizer import features_from_url\n",
    "\n",
    "# Wrapper function (same as your working version)\n",
    "def to_aligned_df(url: str) -> pd.DataFrame:\n",
    "    out = features_from_url(url)\n",
    "    X = pd.DataFrame([out]) if isinstance(out, dict) else out.copy()\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = X[\"TLD\"].astype(str).str.lower().apply(\n",
    "        lambda t: t if t in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "    )\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    num_cols = [c for c in FEATURE_COLS if c != \"TLD\"]\n",
    "    X[num_cols] = X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return X\n",
    "\n",
    "# Prediction function\n",
    "def predict_url(url):\n",
    "    try:\n",
    "        X = to_aligned_df(url)\n",
    "        probability = float(cal.predict_proba(X)[:, 1][0])\n",
    "        return probability\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Model loaded and ready for UI integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e1ddbaf2-dbbe-4db9-979d-a68643cf80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model + schema\n",
      "Imported featureizer from: C:\\Python313\\Scripts\\ AI Phishing & Malicious Link Analyzer\\scripts\n",
      "Schema/dtype checks: OK\n",
      "HR threshold (demo): 0.20\n",
      "https://www.google.com/\n",
      "  prob=0.1170 | Std=benign | HR=benign\n",
      "http://192.168.1.50/login.php?user=guest\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "https://secure-paypa1.com.verify-account.co/reset?session=abc123\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\n",
      "  prob=0.0000 | Std=benign | HR=benign\n",
      "https://www.uni-mainz.de/\n",
      "  prob=0.9871 | Std=PHISHING | HR=PHISHING\n",
      "Total latency: 159.7 ms for 5 URLs\n",
      "\n",
      "============================================================\n",
      "🎯 LIVE PHISHGUARD INTERFACE\n",
      "============================================================\n",
      "\n",
      "📱 INTERACTIVE PHISHING DETECTOR\n",
      "Enter URL and click Analyze:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51441f6a35ee44e489f7f561f3255ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='https://secure-paypa1.com.verify-account.co', description='URL:', layout=Layout(width='80%'), plac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb8371ff5c240c8ada97aecf3ababaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='🔍 Analyze URL', layout=Layout(height='40px', width='200px'), style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d362f5dc9f41c984dd917e83d1a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 QUICK TEST URLS:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c31b7f8598547bcb79d9e9a99f93d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test: Safe', layout=Layout(margin='5px', width='200px'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c0e2d9495d4483b3f4c92bd67b453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test: Suspicious', layout=Layout(margin='5px', width='200px'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c07e635cf8e48e4ae3d5fcf1451ec5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test: High Risk', layout=Layout(margin='5px', width='200px'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac0aabbba134ce397629bd35c12afbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test: Phishing - should be 0.9871', layout=Layout(margin='5px', width='200px'), style=Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233daa3726db41609a846faf039bd55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test: Safe', layout=Layout(margin='5px', width='200px'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Auto-analyzing test URL...\n"
     ]
    }
   ],
   "source": [
    "# === Final Model Smoke Test (Jupyter) ===\n",
    "import sys, pathlib, importlib, json, joblib, pandas as pd, numpy as np, time\n",
    "\n",
    "# 1) Resolve paths: set repo root and common folders\n",
    "ROOT = pathlib.Path(\".\").resolve()  # if notebook is nested, adjust: pathlib.Path(\"..\").resolve()\n",
    "MODELS = ROOT / \"models\"\n",
    "SCRIPTS = ROOT / \"scripts\"\n",
    "APP = ROOT / \"app\"\n",
    "\n",
    "assert (MODELS / \"schema.json\").exists(), f\"Missing: {MODELS/'schema.json'}\"\n",
    "assert (MODELS / \"phish_url_hgb_cal.joblib\").exists(), f\"Missing: {MODELS/'phish_url_hgb_cal.joblib'}\"\n",
    "\n",
    "# 2) Load schema + model\n",
    "SCHEMA = json.load(open(MODELS / \"schema.json\",\"r\",encoding=\"utf-8\"))\n",
    "FEATURE_COLS = SCHEMA[\"feature_cols\"]\n",
    "TLD_CATS = SCHEMA[\"tld_categories\"]\n",
    "cal = joblib.load(MODELS / \"phish_url_hgb_cal.joblib\")\n",
    "print(\"Loaded model + schema\")\n",
    "\n",
    "# 3) Import featureizer: try scripts/featureizer.py then app/featureizer.py\n",
    "FZ = None\n",
    "for candidate in [SCRIPTS, APP]:\n",
    "    if candidate.exists():\n",
    "        if str(candidate) not in sys.path:\n",
    "            sys.path.insert(0, str(candidate))\n",
    "        try:\n",
    "            import featureizer as _tmp\n",
    "            importlib.reload(_tmp)\n",
    "            FZ = _tmp\n",
    "            print(f\"Imported featureizer from: {candidate}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "assert FZ is not None, \"Could not import featureizer.py from scripts/ or app/\"\n",
    "\n",
    "assert hasattr(FZ, \"features_from_url\"), \"featureizer.features_from_url not found\"\n",
    "\n",
    "# 4) Wrapper to enforce schema order + dtypes (parity guard)\n",
    "def to_aligned_df(url: str) -> pd.DataFrame:\n",
    "    out = FZ.features_from_url(url)\n",
    "    X = pd.DataFrame([out]) if isinstance(out, dict) else out.copy()\n",
    "    # add missing and enforce order\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[FEATURE_COLS]\n",
    "    # numerics then categorical\n",
    "    for c in FEATURE_COLS:\n",
    "        if c != \"TLD\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n",
    "    X[\"TLD\"] = X[\"TLD\"].astype(str).str.lower().apply(\n",
    "        lambda t: t if t in TLD_CATS else (\"other\" if \"other\" in TLD_CATS else TLD_CATS[0])\n",
    "    )\n",
    "    X[\"TLD\"] = pd.Categorical(X[\"TLD\"], categories=TLD_CATS)\n",
    "    # final numeric safety\n",
    "    num_cols = [c for c in FEATURE_COLS if c != \"TLD\"]\n",
    "    X[num_cols] = X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return X\n",
    "\n",
    "# 5) Schema/dtype sanity\n",
    "X0 = to_aligned_df(\"https://example.com/?q=1\")\n",
    "assert list(X0.columns) == FEATURE_COLS, \"Column order mismatch\"\n",
    "for c in FEATURE_COLS:\n",
    "    if c != \"TLD\":\n",
    "        assert pd.api.types.is_numeric_dtype(X0[c]), f\"{c} not numeric\"\n",
    "assert pd.api.types.is_categorical_dtype(X0[\"TLD\"]), \"TLD not categorical\"\n",
    "print(\"Schema/dtype checks: OK\")\n",
    "\n",
    "# 6) Choose a demo-friendly High Recall threshold (saved one was ~0)\n",
    "HR_THRESHOLD = 0.20\n",
    "print(f\"HR threshold (demo): {HR_THRESHOLD:.2f}\")\n",
    "\n",
    "# 7) Score a small set\n",
    "tests = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"http://192.168.1.50/login.php?user=guest\",\n",
    "    \"https://secure-paypa1.com.verify-account.co/reset?session=abc123\",\n",
    "    \"http://example.com/%2F%2E%2E/redirect?to=http://bad.ru\",\n",
    "    \"https://www.uni-mainz.de/\",\n",
    "]\n",
    "t0 = time.time()\n",
    "for u in tests:\n",
    "    X = to_aligned_df(u)\n",
    "    p = float(cal.predict_proba(X)[:,1][0])\n",
    "    std = \"PHISHING\" if p>=0.5 else \"benign\"\n",
    "    hr  = \"PHISHING\" if p>=HR_THRESHOLD else \"benign\"\n",
    "    print(f\"{u}\\n  prob={p:.4f} | Std={std} | HR={hr}\")\n",
    "print(f\"Total latency: {(time.time()-t0)*1000:.1f} ms for {len(tests)} URLs\")\n",
    "\n",
    "# === FIXED INTERFACE WITH BUTTON ===\n",
    "from ipywidgets import interact_manual, widgets, Layout, Button, Output\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 LIVE PHISHGUARD INTERFACE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create widgets\n",
    "url_input = widgets.Text(\n",
    "    value='https://secure-paypa1.com.verify-account.co',\n",
    "    placeholder='Enter URL to analyze...',\n",
    "    description='URL:',\n",
    "    layout=Layout(width='80%')\n",
    ")\n",
    "\n",
    "analyze_button = Button(\n",
    "    description=\"🔍 Analyze URL\",\n",
    "    button_style='primary',\n",
    "    layout=Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "output = Output()\n",
    "\n",
    "def on_analyze_click(button):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        url = url_input.value\n",
    "        if not url:\n",
    "            print(\"❌ Please enter a URL\")\n",
    "            return\n",
    "            \n",
    "        X = to_aligned_df(url)\n",
    "        p = float(cal.predict_proba(X)[:,1][0])\n",
    "        \n",
    "        # Calculate verdict\n",
    "        if p >= 0.5:\n",
    "            verdict = \"🛑 MALICIOUS\"\n",
    "            color = \"red\"\n",
    "            risk_level = \"HIGH RISK\"\n",
    "        elif p >= 0.2:\n",
    "            verdict = \"⚠️ SUSPICIOUS\" \n",
    "            color = \"orange\"\n",
    "            risk_level = \"MEDIUM RISK\"\n",
    "        else:\n",
    "            verdict = \"✅ SAFE\"\n",
    "            color = \"green\"\n",
    "            risk_level = \"LOW RISK\"\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n📊 ANALYSIS RESULTS:\")\n",
    "        print(f\"🔗 URL: {url}\")\n",
    "        print(f\"🎯 Risk Score: {p:.4f}\")\n",
    "        print(f\"📈 Risk Level: {risk_level}\")\n",
    "        print(f\"⚖️ Verdict: {verdict}\")\n",
    "        print(f\"💪 Confidence: {max(p, 1-p)*100:.1f}%\")\n",
    "        \n",
    "        # Simple gauge visualization\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.barh([0], [p], color=color, alpha=0.7, height=0.5)\n",
    "        plt.barh([0], [1-p], left=[p], color='lightgray', alpha=0.5, height=0.5)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.title(f'Phishing Risk: {p*100:.1f}%', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add risk labels\n",
    "        plt.text(0.1, 0, 'SAFE', ha='center', va='center', fontweight='bold')\n",
    "        plt.text(0.9, 0, 'MALICIOUS', ha='center', va='center', fontweight='bold')\n",
    "        plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Risk factors\n",
    "        print(f\"\\n🔍 RISK FACTORS:\")\n",
    "        if p > 0.7:\n",
    "            factors = [\n",
    "                \"• High domain suspiciousness\",\n",
    "                \"• Multiple phishing indicators\", \n",
    "                \"• Unusual URL structure\",\n",
    "                \"• Likely malicious destination\"\n",
    "            ]\n",
    "        elif p > 0.4:\n",
    "            factors = [\n",
    "                \"• Moderate risk indicators\",\n",
    "                \"• Some suspicious elements\",\n",
    "                \"• Verify before clicking\",\n",
    "                \"• Potential security risk\"\n",
    "            ]\n",
    "        else:\n",
    "            factors = [\n",
    "                \"• Clean URL structure\",\n",
    "                \"• Trusted domain patterns\", \n",
    "                \"• Low risk indicators\",\n",
    "                \"• Likely legitimate website\"\n",
    "            ]\n",
    "        \n",
    "        for factor in factors:\n",
    "            print(factor)\n",
    "\n",
    "# Connect button\n",
    "analyze_button.on_click(on_analyze_click)\n",
    "\n",
    "# Display interface\n",
    "print(\"\\n📱 INTERACTIVE PHISHING DETECTOR\")\n",
    "print(\"Enter URL and click Analyze:\")\n",
    "display(url_input)\n",
    "display(analyze_button)\n",
    "display(output)\n",
    "\n",
    "print(\"\\n💡 QUICK TEST URLS:\")\n",
    "test_urls = [\n",
    "    (\"https://www.google.com/\", \"Safe\"),\n",
    "    (\"https://secure-paypa1.com.verify-account.co\", \"Suspicious\"),\n",
    "    (\"http://192.168.1.1/login.php\", \"High Risk\"), \n",
    "    (\"https://www.uni-mainz.de/\", \"Phishing - should be 0.9871\"),\n",
    "    (\"https://www.paypal.com/\", \"Safe\")\n",
    "]\n",
    "\n",
    "for url, desc in test_urls:\n",
    "    test_btn = Button(description=f\"Test: {desc}\", layout=Layout(width='200px', margin='5px'))\n",
    "    test_btn.on_click(lambda x, u=url: (setattr(url_input, 'value', u), on_analyze_click(None)))\n",
    "    display(test_btn)\n",
    "\n",
    "# Auto-analyze the first test URL\n",
    "print(\"\\n🔄 Auto-analyzing test URL...\")\n",
    "url_input.value = \"https://www.uni-mainz.de/\"\n",
    "on_analyze_click(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894750c-12a8-485d-96a6-2b79d9e40b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
